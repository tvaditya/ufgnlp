{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attetnion_Translator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPSWmth6aO/UgAs7a2I1G/b"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_P_QAIjJ_dY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Importing libraries\n",
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx994FtfPOBW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2482e10a-1a21-443e-bfc0-52ddca2c6306"
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3vb6MOSRYtp",
        "colab_type": "code",
        "outputId": "51b87a0e-0979-44a7-e2cb-397363a3e95d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImPdNp5yea_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/content/drive/My Drive/nlp/pt-en/europarl-v7.pt-en.en\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    europarl_en = f.read()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvJq392lfwuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/content/drive/My Drive/nlp/pt-en/europarl-v7.pt-en.pt\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    europarl_pt = f.read()\n",
        "with open(\"/content/drive/My Drive/nlp/pt-en/nonbreaking_prefix.en\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    non_breaking_prefix_en = f.read()\n",
        "with open(\"/content/drive/My Drive/nlp/pt-en/nonbreaking_prefix.pt\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    non_breaking_prefix_pt = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjKOz_3N0tpa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Cleaning data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q6vHdudj0xc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
        "non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hoowkn_-0r3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "non_breaking_prefix_pt = non_breaking_prefix_pt.split(\"\\n\")\n",
        "non_breaking_prefix_pt = [' ' + pref + '.' for pref in non_breaking_prefix_pt]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZskEHZq07AL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# non_breaking_prefix_en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gS_kGE81P-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will need ach word and symbol that we want t o keep to be in lower case and separated by spaces so we can tokenize them\n",
        "# First we will replace the . in our non_breking prefix list by ###\n",
        "corpus_en = europarl_en\n",
        "for prefix in non_breaking_prefix_en:\n",
        "  corpus_en = corpus_en.replace(prefix, prefix + \"###\")\n",
        "\n",
        "# you may have a .followed by number or letter\n",
        "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".###\", corpus_en)\n",
        "corpus_en = re.sub(r\"\\.###\", '', corpus_en)\n",
        "corpus_en = re.sub(r\"  +\", ' ', corpus_en)\n",
        "corpus_en = corpus_en.split(\"\\n\")\n",
        "\n",
        "# same for pt\n",
        "corpus_pt = europarl_pt\n",
        "for prefix in non_breaking_prefix_pt:\n",
        "  corpus_pt = corpus_pt.replace(prefix, prefix + \"###\")\n",
        "\n",
        "# you may have a .followed by number or letter\n",
        "corpus_pt = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".###\", corpus_pt)\n",
        "corpus_pt = re.sub(r\"\\.###\", '', corpus_pt)\n",
        "corpus_pt = re.sub(r\"  +\", ' ', corpus_pt)\n",
        "corpus_pt = corpus_pt.split(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSQ7crCn6dW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize\n",
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_en, target_vocab_size=2**13)\n",
        "tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_pt, target_vocab_size=2**13)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c2uIeDb8OpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2\n",
        "VOCAB_SIZE_PT = tokenizer_pt.vocab_size + 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh9SCZg08Ul-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Each sentence will start with the starting_token and end with the ending_token\n",
        "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
        "          for sentence in corpus_en]\n",
        "outputs = [[VOCAB_SIZE_PT-2] + tokenizer_pt.encode(sentence) + [VOCAB_SIZE_PT-1]\n",
        "           for sentence in corpus_pt]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJtaDam0Aj-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove to long sentences\n",
        "MAX_LENGTH = 20\n",
        "\n",
        "idx_to_remove = [count for count,sent in enumerate(inputs) if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "  del inputs[idx]\n",
        "  del outputs[idx]\n",
        "\n",
        "idx_to_remove = [count for count,sent in enumerate(outputs) if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "  del inputs[idx]\n",
        "  del outputs[idx]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuvJJ6b7FSK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inputs creation\n",
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=MAX_LENGTH)\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=MAX_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9otFMTqYxVJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a8f4f069-6d93-4997-bee3-5d48e83e51d8"
      },
      "source": [
        "list(inputs.shape)[-2]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "396108"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1_IeeS7F2JW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOfvABb0HWH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Model building"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut3kcHZFHZld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Positional encoding\n",
        "class PositionalEncoding(layers.Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "    \n",
        "    def get_angles(self, pos, i, d_model): # pos: (seq_length, 1) i: (1, d_model)\n",
        "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
        "        return pos * angles #shape: (seq_length, d_model)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        #seq_length = inputs.shape.as_list()[-2]\n",
        "        seq_length = list(inputs.shape)[-2]\n",
        "        #d_model = inputs.shape.as_list()[-1]\n",
        "        d_model = list(inputs.shape)[-1]\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                                 np.arange(d_model)[np.newaxis, :],\n",
        "                                 d_model)\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "        pos_encoding = angles[np.newaxis, ...] #because of Batch\n",
        "        return inputs + tf.cast(pos_encoding, tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWkHVhwKTQNr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scaled dot product attention\n",
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "    product = tf.matmul(queries, keys, transpose_b=True)\n",
        "    \n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
        "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "    \n",
        "    if mask is not None:\n",
        "        scaled_product += (mask * -1e9)\n",
        "    \n",
        "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
        "    \n",
        "    return attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QouAwTdgZzKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    \n",
        "    def __init__(self, nb_proj):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.nb_proj = nb_proj\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        assert self.d_model % self.nb_proj == 0\n",
        "        \n",
        "        self.d_proj = self.d_model // self.nb_proj\n",
        "        \n",
        "        self.query_lin = layers.Dense(units=self.d_model)\n",
        "        self.key_lin = layers.Dense(units=self.d_model)\n",
        "        self.value_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "        self.final_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n",
        "        shape = (batch_size,\n",
        "                 -1,\n",
        "                 self.nb_proj,\n",
        "                 self.d_proj)\n",
        "        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n",
        "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n",
        "    \n",
        "    def call(self, queries, keys, values, mask):\n",
        "        batch_size = tf.shape(queries)[0]\n",
        "        \n",
        "        queries = self.query_lin(queries)\n",
        "        keys = self.key_lin(keys)\n",
        "        values = self.value_lin(values)\n",
        "        \n",
        "        queries = self.split_proj(queries, batch_size)\n",
        "        keys = self.split_proj(keys, batch_size)\n",
        "        values = self.split_proj(values, batch_size)\n",
        "        \n",
        "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "        \n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        \n",
        "        concat_attention = tf.reshape(attention,\n",
        "                                      shape=(batch_size, -1, self.d_model))\n",
        "        \n",
        "        outputs = self.final_lin(concat_attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6A_T5gZbqz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encoder\n",
        "\n",
        "class EncoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, mask, training):\n",
        "        attention = self.multi_head_attention(inputs,\n",
        "                                              inputs,\n",
        "                                              inputs,\n",
        "                                              mask)\n",
        "        attention = self.dropout_1(attention, training=training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        outputs = self.dense_1(attention)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_2(outputs, training=training)\n",
        "        outputs = self.norm_2(outputs + attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QS_BTegfKIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"encoder\"):\n",
        "        super(Encoder, self).__init__(name=name)\n",
        "        self.nb_layers = nb_layers\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        self.enc_layers = [EncoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate) \n",
        "                           for _ in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, mask, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.enc_layers[i](outputs, mask, training)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0JsPpc1fNZk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decoder\n",
        "class DecoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        # Self multi head attention\n",
        "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Multi head attention combined with encoder output\n",
        "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Feed foward\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units,\n",
        "                                    activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        attention = self.multi_head_attention_1(inputs,\n",
        "                                                inputs,\n",
        "                                                inputs,\n",
        "                                                mask_1)\n",
        "        attention = self.dropout_1(attention, training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        attention_2 = self.multi_head_attention_2(attention,\n",
        "                                                  enc_outputs,\n",
        "                                                  enc_outputs,\n",
        "                                                  mask_2)\n",
        "        attention_2 = self.dropout_2(attention_2, training)\n",
        "        attention_2 = self.norm_2(attention_2 + attention)\n",
        "        \n",
        "        outputs = self.dense_1(attention_2)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_3(outputs, training)\n",
        "        outputs = self.norm_3(outputs + attention_2)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LccE_ITZfUG4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"decoder\"):\n",
        "        super(Decoder, self).__init__(name=name)\n",
        "        self.d_model = d_model\n",
        "        self.nb_layers = nb_layers\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        \n",
        "        self.dec_layers = [DecoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate) \n",
        "                           for i in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.dec_layers[i](outputs,\n",
        "                                         enc_outputs,\n",
        "                                         mask_1,\n",
        "                                         mask_2,\n",
        "                                         training)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbHynSCZfaKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transformer\n",
        "class Transformer(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 vocab_size_dec,\n",
        "                 d_model,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 name=\"transformer\"):\n",
        "        super(Transformer, self).__init__(name=name)\n",
        "        \n",
        "        self.encoder = Encoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_enc,\n",
        "                               d_model)\n",
        "        self.decoder = Decoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_dec,\n",
        "                               d_model)\n",
        "        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n",
        "    \n",
        "    def create_padding_mask(self, seq):\n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    def create_look_ahead_mask(self, seq):\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        return look_ahead_mask\n",
        "    \n",
        "    def call(self, enc_inputs, dec_inputs, training):\n",
        "        enc_mask = self.create_padding_mask(enc_inputs)\n",
        "        dec_mask_1 = tf.maximum(\n",
        "            self.create_padding_mask(dec_inputs),\n",
        "            self.create_look_ahead_mask(dec_inputs)\n",
        "        )\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "        \n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "        dec_outputs = self.decoder(dec_inputs,\n",
        "                                   enc_outputs,\n",
        "                                   dec_mask_1,\n",
        "                                   dec_mask_2,\n",
        "                                   training)\n",
        "        \n",
        "        outputs = self.last_linear(dec_outputs)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYMQLI_UhFnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## TRAINING"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HogRy7tyfiJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Hyper-parameters\n",
        "D_MODEL = 128 # 512\n",
        "NB_LAYERS = 4 # 6\n",
        "FFN_UNITS = 512 # 2048\n",
        "NB_PROJ = 8 # 8\n",
        "DROPOUT_RATE = 0.1 # 0.1\n",
        "\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec=VOCAB_SIZE_PT,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WYAIfAyg9-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction=\"none\")\n",
        "\n",
        "def loss_function(target, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "    loss_ = loss_object(target, pred)\n",
        "    \n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    \n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH2QdTwYhLtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "        \n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "leaning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(leaning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jE5g0ejchQDK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7478cd9b-2915-4988-f390-d903f615e241"
      },
      "source": [
        "checkpoint_path = \"./drive/My Drive/nlp/ckpt/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!!\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Latest checkpoint restored!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLxII2GOhTV8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "908e3dab-5c1f-4820-8f5d-14d1e9134753"
      },
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"Start of epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "        dec_inputs = targets[:, :-1]\n",
        "        dec_outputs_real = targets[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "            loss = loss_function(dec_outputs_real, predictions)\n",
        "        \n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "        \n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs_real, predictions)\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
        "            \n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\n",
        "                                                        ckpt_save_path))\n",
        "    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Batch 0 Loss 3.1200 Accuracy 0.1949\n",
            "Epoch 1 Batch 50 Loss 3.1665 Accuracy 0.2070\n",
            "Epoch 1 Batch 100 Loss 3.1210 Accuracy 0.2091\n",
            "Epoch 1 Batch 150 Loss 3.0854 Accuracy 0.2105\n",
            "Epoch 1 Batch 200 Loss 3.0694 Accuracy 0.2120\n",
            "Epoch 1 Batch 250 Loss 3.0556 Accuracy 0.2135\n",
            "Epoch 1 Batch 300 Loss 3.0377 Accuracy 0.2150\n",
            "Epoch 1 Batch 350 Loss 3.0194 Accuracy 0.2168\n",
            "Epoch 1 Batch 400 Loss 2.9994 Accuracy 0.2184\n",
            "Epoch 1 Batch 450 Loss 2.9822 Accuracy 0.2195\n",
            "Epoch 1 Batch 500 Loss 2.9638 Accuracy 0.2206\n",
            "Epoch 1 Batch 550 Loss 2.9489 Accuracy 0.2218\n",
            "Epoch 1 Batch 600 Loss 2.9348 Accuracy 0.2230\n",
            "Epoch 1 Batch 650 Loss 2.9198 Accuracy 0.2244\n",
            "Epoch 1 Batch 700 Loss 2.9070 Accuracy 0.2255\n",
            "Epoch 1 Batch 750 Loss 2.8971 Accuracy 0.2266\n",
            "Epoch 1 Batch 800 Loss 2.8845 Accuracy 0.2276\n",
            "Epoch 1 Batch 850 Loss 2.8689 Accuracy 0.2288\n",
            "Epoch 1 Batch 900 Loss 2.8574 Accuracy 0.2297\n",
            "Epoch 1 Batch 950 Loss 2.8444 Accuracy 0.2307\n",
            "Epoch 1 Batch 1000 Loss 2.8302 Accuracy 0.2317\n",
            "Epoch 1 Batch 1050 Loss 2.8182 Accuracy 0.2325\n",
            "Epoch 1 Batch 1100 Loss 2.8050 Accuracy 0.2334\n",
            "Epoch 1 Batch 1150 Loss 2.7950 Accuracy 0.2344\n",
            "Epoch 1 Batch 1200 Loss 2.7868 Accuracy 0.2355\n",
            "Epoch 1 Batch 1250 Loss 2.7770 Accuracy 0.2366\n",
            "Epoch 1 Batch 1300 Loss 2.7659 Accuracy 0.2377\n",
            "Epoch 1 Batch 1350 Loss 2.7545 Accuracy 0.2386\n",
            "Epoch 1 Batch 1400 Loss 2.7438 Accuracy 0.2397\n",
            "Epoch 1 Batch 1450 Loss 2.7318 Accuracy 0.2409\n",
            "Epoch 1 Batch 1500 Loss 2.7213 Accuracy 0.2421\n",
            "Epoch 1 Batch 1550 Loss 2.7105 Accuracy 0.2433\n",
            "Epoch 1 Batch 1600 Loss 2.7006 Accuracy 0.2445\n",
            "Epoch 1 Batch 1650 Loss 2.6895 Accuracy 0.2458\n",
            "Epoch 1 Batch 1700 Loss 2.6797 Accuracy 0.2471\n",
            "Epoch 1 Batch 1750 Loss 2.6706 Accuracy 0.2483\n",
            "Epoch 1 Batch 1800 Loss 2.6610 Accuracy 0.2494\n",
            "Epoch 1 Batch 1850 Loss 2.6505 Accuracy 0.2507\n",
            "Epoch 1 Batch 1900 Loss 2.6408 Accuracy 0.2518\n",
            "Epoch 1 Batch 1950 Loss 2.6309 Accuracy 0.2530\n",
            "Epoch 1 Batch 2000 Loss 2.6208 Accuracy 0.2542\n",
            "Epoch 1 Batch 2050 Loss 2.6109 Accuracy 0.2555\n",
            "Epoch 1 Batch 2100 Loss 2.6006 Accuracy 0.2568\n",
            "Epoch 1 Batch 2150 Loss 2.5910 Accuracy 0.2580\n",
            "Epoch 1 Batch 2200 Loss 2.5815 Accuracy 0.2593\n",
            "Epoch 1 Batch 2250 Loss 2.5708 Accuracy 0.2604\n",
            "Epoch 1 Batch 2300 Loss 2.5612 Accuracy 0.2616\n",
            "Epoch 1 Batch 2350 Loss 2.5497 Accuracy 0.2627\n",
            "Epoch 1 Batch 2400 Loss 2.5378 Accuracy 0.2638\n",
            "Epoch 1 Batch 2450 Loss 2.5271 Accuracy 0.2649\n",
            "Epoch 1 Batch 2500 Loss 2.5161 Accuracy 0.2660\n",
            "Epoch 1 Batch 2550 Loss 2.5051 Accuracy 0.2671\n",
            "Epoch 1 Batch 2600 Loss 2.4944 Accuracy 0.2683\n",
            "Epoch 1 Batch 2650 Loss 2.4840 Accuracy 0.2694\n",
            "Epoch 1 Batch 2700 Loss 2.4736 Accuracy 0.2706\n",
            "Epoch 1 Batch 2750 Loss 2.4629 Accuracy 0.2718\n",
            "Epoch 1 Batch 2800 Loss 2.4530 Accuracy 0.2730\n",
            "Epoch 1 Batch 2850 Loss 2.4442 Accuracy 0.2742\n",
            "Epoch 1 Batch 2900 Loss 2.4338 Accuracy 0.2755\n",
            "Epoch 1 Batch 2950 Loss 2.4240 Accuracy 0.2768\n",
            "Epoch 1 Batch 3000 Loss 2.4141 Accuracy 0.2779\n",
            "Epoch 1 Batch 3050 Loss 2.4051 Accuracy 0.2791\n",
            "Epoch 1 Batch 3100 Loss 2.3956 Accuracy 0.2803\n",
            "Epoch 1 Batch 3150 Loss 2.3864 Accuracy 0.2815\n",
            "Epoch 1 Batch 3200 Loss 2.3773 Accuracy 0.2828\n",
            "Epoch 1 Batch 3250 Loss 2.3680 Accuracy 0.2840\n",
            "Epoch 1 Batch 3300 Loss 2.3594 Accuracy 0.2852\n",
            "Epoch 1 Batch 3350 Loss 2.3500 Accuracy 0.2865\n",
            "Epoch 1 Batch 3400 Loss 2.3408 Accuracy 0.2876\n",
            "Epoch 1 Batch 3450 Loss 2.3317 Accuracy 0.2889\n",
            "Epoch 1 Batch 3500 Loss 2.3224 Accuracy 0.2901\n",
            "Epoch 1 Batch 3550 Loss 2.3136 Accuracy 0.2913\n",
            "Epoch 1 Batch 3600 Loss 2.3047 Accuracy 0.2925\n",
            "Epoch 1 Batch 3650 Loss 2.2960 Accuracy 0.2937\n",
            "Epoch 1 Batch 3700 Loss 2.2872 Accuracy 0.2949\n",
            "Epoch 1 Batch 3750 Loss 2.2789 Accuracy 0.2960\n",
            "Epoch 1 Batch 3800 Loss 2.2710 Accuracy 0.2971\n",
            "Epoch 1 Batch 3850 Loss 2.2630 Accuracy 0.2982\n",
            "Epoch 1 Batch 3900 Loss 2.2545 Accuracy 0.2994\n",
            "Epoch 1 Batch 3950 Loss 2.2462 Accuracy 0.3005\n",
            "Epoch 1 Batch 4000 Loss 2.2382 Accuracy 0.3017\n",
            "Epoch 1 Batch 4050 Loss 2.2301 Accuracy 0.3028\n",
            "Epoch 1 Batch 4100 Loss 2.2221 Accuracy 0.3040\n",
            "Epoch 1 Batch 4150 Loss 2.2143 Accuracy 0.3051\n",
            "Epoch 1 Batch 4200 Loss 2.2067 Accuracy 0.3062\n",
            "Epoch 1 Batch 4250 Loss 2.1995 Accuracy 0.3073\n",
            "Epoch 1 Batch 4300 Loss 2.1918 Accuracy 0.3084\n",
            "Epoch 1 Batch 4350 Loss 2.1842 Accuracy 0.3096\n",
            "Epoch 1 Batch 4400 Loss 2.1770 Accuracy 0.3107\n",
            "Epoch 1 Batch 4450 Loss 2.1696 Accuracy 0.3118\n",
            "Epoch 1 Batch 4500 Loss 2.1627 Accuracy 0.3129\n",
            "Epoch 1 Batch 4550 Loss 2.1566 Accuracy 0.3137\n",
            "Epoch 1 Batch 4600 Loss 2.1511 Accuracy 0.3146\n",
            "Epoch 1 Batch 4650 Loss 2.1462 Accuracy 0.3153\n",
            "Epoch 1 Batch 4700 Loss 2.1414 Accuracy 0.3159\n",
            "Epoch 1 Batch 4750 Loss 2.1369 Accuracy 0.3166\n",
            "Epoch 1 Batch 4800 Loss 2.1331 Accuracy 0.3171\n",
            "Epoch 1 Batch 4850 Loss 2.1290 Accuracy 0.3176\n",
            "Epoch 1 Batch 4900 Loss 2.1253 Accuracy 0.3181\n",
            "Epoch 1 Batch 4950 Loss 2.1217 Accuracy 0.3186\n",
            "Epoch 1 Batch 5000 Loss 2.1179 Accuracy 0.3190\n",
            "Epoch 1 Batch 5050 Loss 2.1143 Accuracy 0.3195\n",
            "Epoch 1 Batch 5100 Loss 2.1107 Accuracy 0.3200\n",
            "Epoch 1 Batch 5150 Loss 2.1071 Accuracy 0.3204\n",
            "Epoch 1 Batch 5200 Loss 2.1043 Accuracy 0.3209\n",
            "Epoch 1 Batch 5250 Loss 2.1008 Accuracy 0.3214\n",
            "Epoch 1 Batch 5300 Loss 2.0974 Accuracy 0.3218\n",
            "Epoch 1 Batch 5350 Loss 2.0939 Accuracy 0.3223\n",
            "Epoch 1 Batch 5400 Loss 2.0910 Accuracy 0.3227\n",
            "Epoch 1 Batch 5450 Loss 2.0879 Accuracy 0.3231\n",
            "Epoch 1 Batch 5500 Loss 2.0846 Accuracy 0.3236\n",
            "Epoch 1 Batch 5550 Loss 2.0814 Accuracy 0.3239\n",
            "Epoch 1 Batch 5600 Loss 2.0783 Accuracy 0.3244\n",
            "Epoch 1 Batch 5650 Loss 2.0753 Accuracy 0.3248\n",
            "Epoch 1 Batch 5700 Loss 2.0723 Accuracy 0.3252\n",
            "Epoch 1 Batch 5750 Loss 2.0688 Accuracy 0.3255\n",
            "Epoch 1 Batch 5800 Loss 2.0656 Accuracy 0.3259\n",
            "Epoch 1 Batch 5850 Loss 2.0623 Accuracy 0.3262\n",
            "Epoch 1 Batch 5900 Loss 2.0592 Accuracy 0.3266\n",
            "Epoch 1 Batch 5950 Loss 2.0561 Accuracy 0.3270\n",
            "Epoch 1 Batch 6000 Loss 2.0530 Accuracy 0.3273\n",
            "Epoch 1 Batch 6050 Loss 2.0500 Accuracy 0.3277\n",
            "Epoch 1 Batch 6100 Loss 2.0467 Accuracy 0.3281\n",
            "Epoch 1 Batch 6150 Loss 2.0435 Accuracy 0.3284\n",
            "Saving checkpoint for epoch 1 at ./drive/My Drive/nlp/ckpt/ckpt-2\n",
            "Time taken for 1 epoch: 1813.31298494339 secs\n",
            "\n",
            "Start of epoch 2\n",
            "Epoch 2 Batch 0 Loss 1.7800 Accuracy 0.3849\n",
            "Epoch 2 Batch 50 Loss 1.7016 Accuracy 0.3763\n",
            "Epoch 2 Batch 100 Loss 1.6871 Accuracy 0.3777\n",
            "Epoch 2 Batch 150 Loss 1.6834 Accuracy 0.3795\n",
            "Epoch 2 Batch 200 Loss 1.6775 Accuracy 0.3803\n",
            "Epoch 2 Batch 250 Loss 1.6725 Accuracy 0.3807\n",
            "Epoch 2 Batch 300 Loss 1.6697 Accuracy 0.3816\n",
            "Epoch 2 Batch 350 Loss 1.6675 Accuracy 0.3827\n",
            "Epoch 2 Batch 400 Loss 1.6604 Accuracy 0.3832\n",
            "Epoch 2 Batch 450 Loss 1.6528 Accuracy 0.3834\n",
            "Epoch 2 Batch 500 Loss 1.6465 Accuracy 0.3839\n",
            "Epoch 2 Batch 550 Loss 1.6410 Accuracy 0.3841\n",
            "Epoch 2 Batch 600 Loss 1.6373 Accuracy 0.3840\n",
            "Epoch 2 Batch 650 Loss 1.6343 Accuracy 0.3842\n",
            "Epoch 2 Batch 700 Loss 1.6296 Accuracy 0.3845\n",
            "Epoch 2 Batch 750 Loss 1.6239 Accuracy 0.3851\n",
            "Epoch 2 Batch 800 Loss 1.6188 Accuracy 0.3858\n",
            "Epoch 2 Batch 850 Loss 1.6124 Accuracy 0.3866\n",
            "Epoch 2 Batch 900 Loss 1.6063 Accuracy 0.3872\n",
            "Epoch 2 Batch 950 Loss 1.6009 Accuracy 0.3879\n",
            "Epoch 2 Batch 1000 Loss 1.5951 Accuracy 0.3881\n",
            "Epoch 2 Batch 1050 Loss 1.5907 Accuracy 0.3885\n",
            "Epoch 2 Batch 1100 Loss 1.5844 Accuracy 0.3888\n",
            "Epoch 2 Batch 1150 Loss 1.5789 Accuracy 0.3897\n",
            "Epoch 2 Batch 1200 Loss 1.5739 Accuracy 0.3904\n",
            "Epoch 2 Batch 1250 Loss 1.5688 Accuracy 0.3913\n",
            "Epoch 2 Batch 1300 Loss 1.5637 Accuracy 0.3919\n",
            "Epoch 2 Batch 1350 Loss 1.5592 Accuracy 0.3927\n",
            "Epoch 2 Batch 1400 Loss 1.5542 Accuracy 0.3936\n",
            "Epoch 2 Batch 1450 Loss 1.5484 Accuracy 0.3943\n",
            "Epoch 2 Batch 1500 Loss 1.5439 Accuracy 0.3950\n",
            "Epoch 2 Batch 1550 Loss 1.5408 Accuracy 0.3957\n",
            "Epoch 2 Batch 1600 Loss 1.5368 Accuracy 0.3965\n",
            "Epoch 2 Batch 1650 Loss 1.5318 Accuracy 0.3972\n",
            "Epoch 2 Batch 1700 Loss 1.5281 Accuracy 0.3979\n",
            "Epoch 2 Batch 1750 Loss 1.5245 Accuracy 0.3987\n",
            "Epoch 2 Batch 1800 Loss 1.5197 Accuracy 0.3995\n",
            "Epoch 2 Batch 1850 Loss 1.5163 Accuracy 0.4002\n",
            "Epoch 2 Batch 1900 Loss 1.5125 Accuracy 0.4009\n",
            "Epoch 2 Batch 1950 Loss 1.5087 Accuracy 0.4016\n",
            "Epoch 2 Batch 2000 Loss 1.5045 Accuracy 0.4023\n",
            "Epoch 2 Batch 2050 Loss 1.5007 Accuracy 0.4029\n",
            "Epoch 2 Batch 2100 Loss 1.4975 Accuracy 0.4035\n",
            "Epoch 2 Batch 2150 Loss 1.4936 Accuracy 0.4040\n",
            "Epoch 2 Batch 2200 Loss 1.4896 Accuracy 0.4043\n",
            "Epoch 2 Batch 2250 Loss 1.4850 Accuracy 0.4048\n",
            "Epoch 2 Batch 2300 Loss 1.4802 Accuracy 0.4052\n",
            "Epoch 2 Batch 2350 Loss 1.4754 Accuracy 0.4056\n",
            "Epoch 2 Batch 2400 Loss 1.4708 Accuracy 0.4059\n",
            "Epoch 2 Batch 2450 Loss 1.4658 Accuracy 0.4063\n",
            "Epoch 2 Batch 2500 Loss 1.4620 Accuracy 0.4067\n",
            "Epoch 2 Batch 2550 Loss 1.4576 Accuracy 0.4070\n",
            "Epoch 2 Batch 2600 Loss 1.4537 Accuracy 0.4075\n",
            "Epoch 2 Batch 2650 Loss 1.4500 Accuracy 0.4078\n",
            "Epoch 2 Batch 2700 Loss 1.4464 Accuracy 0.4082\n",
            "Epoch 2 Batch 2750 Loss 1.4430 Accuracy 0.4086\n",
            "Epoch 2 Batch 2800 Loss 1.4397 Accuracy 0.4090\n",
            "Epoch 2 Batch 2850 Loss 1.4366 Accuracy 0.4094\n",
            "Epoch 2 Batch 2900 Loss 1.4332 Accuracy 0.4098\n",
            "Epoch 2 Batch 2950 Loss 1.4300 Accuracy 0.4102\n",
            "Epoch 2 Batch 3000 Loss 1.4263 Accuracy 0.4108\n",
            "Epoch 2 Batch 3050 Loss 1.4231 Accuracy 0.4113\n",
            "Epoch 2 Batch 3100 Loss 1.4203 Accuracy 0.4117\n",
            "Epoch 2 Batch 3150 Loss 1.4171 Accuracy 0.4122\n",
            "Epoch 2 Batch 3200 Loss 1.4136 Accuracy 0.4126\n",
            "Epoch 2 Batch 3250 Loss 1.4104 Accuracy 0.4131\n",
            "Epoch 2 Batch 3300 Loss 1.4078 Accuracy 0.4135\n",
            "Epoch 2 Batch 3350 Loss 1.4048 Accuracy 0.4140\n",
            "Epoch 2 Batch 3400 Loss 1.4018 Accuracy 0.4146\n",
            "Epoch 2 Batch 3450 Loss 1.3986 Accuracy 0.4151\n",
            "Epoch 2 Batch 3500 Loss 1.3954 Accuracy 0.4155\n",
            "Epoch 2 Batch 3550 Loss 1.3924 Accuracy 0.4160\n",
            "Epoch 2 Batch 3600 Loss 1.3893 Accuracy 0.4164\n",
            "Epoch 2 Batch 3650 Loss 1.3866 Accuracy 0.4169\n",
            "Epoch 2 Batch 3700 Loss 1.3837 Accuracy 0.4173\n",
            "Epoch 2 Batch 3750 Loss 1.3811 Accuracy 0.4177\n",
            "Epoch 2 Batch 3800 Loss 1.3783 Accuracy 0.4181\n",
            "Epoch 2 Batch 3850 Loss 1.3756 Accuracy 0.4186\n",
            "Epoch 2 Batch 3900 Loss 1.3727 Accuracy 0.4191\n",
            "Epoch 2 Batch 3950 Loss 1.3700 Accuracy 0.4196\n",
            "Epoch 2 Batch 4000 Loss 1.3675 Accuracy 0.4199\n",
            "Epoch 2 Batch 4050 Loss 1.3651 Accuracy 0.4204\n",
            "Epoch 2 Batch 4100 Loss 1.3622 Accuracy 0.4209\n",
            "Epoch 2 Batch 4150 Loss 1.3599 Accuracy 0.4214\n",
            "Epoch 2 Batch 4200 Loss 1.3575 Accuracy 0.4219\n",
            "Epoch 2 Batch 4250 Loss 1.3551 Accuracy 0.4223\n",
            "Epoch 2 Batch 4300 Loss 1.3525 Accuracy 0.4228\n",
            "Epoch 2 Batch 4350 Loss 1.3502 Accuracy 0.4233\n",
            "Epoch 2 Batch 4400 Loss 1.3477 Accuracy 0.4237\n",
            "Epoch 2 Batch 4450 Loss 1.3455 Accuracy 0.4242\n",
            "Epoch 2 Batch 4500 Loss 1.3435 Accuracy 0.4245\n",
            "Epoch 2 Batch 4550 Loss 1.3423 Accuracy 0.4247\n",
            "Epoch 2 Batch 4600 Loss 1.3418 Accuracy 0.4250\n",
            "Epoch 2 Batch 4650 Loss 1.3420 Accuracy 0.4250\n",
            "Epoch 2 Batch 4700 Loss 1.3420 Accuracy 0.4250\n",
            "Epoch 2 Batch 4750 Loss 1.3419 Accuracy 0.4250\n",
            "Epoch 2 Batch 4800 Loss 1.3423 Accuracy 0.4250\n",
            "Epoch 2 Batch 4850 Loss 1.3431 Accuracy 0.4249\n",
            "Epoch 2 Batch 4900 Loss 1.3440 Accuracy 0.4248\n",
            "Epoch 2 Batch 4950 Loss 1.3447 Accuracy 0.4246\n",
            "Epoch 2 Batch 5000 Loss 1.3458 Accuracy 0.4244\n",
            "Epoch 2 Batch 5050 Loss 1.3468 Accuracy 0.4243\n",
            "Epoch 2 Batch 5100 Loss 1.3479 Accuracy 0.4242\n",
            "Epoch 2 Batch 5150 Loss 1.3489 Accuracy 0.4240\n",
            "Epoch 2 Batch 5200 Loss 1.3497 Accuracy 0.4239\n",
            "Epoch 2 Batch 5250 Loss 1.3507 Accuracy 0.4238\n",
            "Epoch 2 Batch 5300 Loss 1.3513 Accuracy 0.4237\n",
            "Epoch 2 Batch 5350 Loss 1.3525 Accuracy 0.4235\n",
            "Epoch 2 Batch 5400 Loss 1.3533 Accuracy 0.4235\n",
            "Epoch 2 Batch 5450 Loss 1.3544 Accuracy 0.4233\n",
            "Epoch 2 Batch 5500 Loss 1.3557 Accuracy 0.4232\n",
            "Epoch 2 Batch 5550 Loss 1.3563 Accuracy 0.4230\n",
            "Epoch 2 Batch 5600 Loss 1.3572 Accuracy 0.4229\n",
            "Epoch 2 Batch 5650 Loss 1.3580 Accuracy 0.4228\n",
            "Epoch 2 Batch 5700 Loss 1.3590 Accuracy 0.4226\n",
            "Epoch 2 Batch 5750 Loss 1.3598 Accuracy 0.4225\n",
            "Epoch 2 Batch 5800 Loss 1.3607 Accuracy 0.4223\n",
            "Epoch 2 Batch 5850 Loss 1.3614 Accuracy 0.4221\n",
            "Epoch 2 Batch 5900 Loss 1.3622 Accuracy 0.4220\n",
            "Epoch 2 Batch 5950 Loss 1.3628 Accuracy 0.4219\n",
            "Epoch 2 Batch 6000 Loss 1.3633 Accuracy 0.4217\n",
            "Epoch 2 Batch 6050 Loss 1.3637 Accuracy 0.4216\n",
            "Epoch 2 Batch 6100 Loss 1.3641 Accuracy 0.4215\n",
            "Epoch 2 Batch 6150 Loss 1.3646 Accuracy 0.4214\n",
            "Saving checkpoint for epoch 2 at ./drive/My Drive/nlp/ckpt/ckpt-3\n",
            "Time taken for 1 epoch: 1771.635710477829 secs\n",
            "\n",
            "Start of epoch 3\n",
            "Epoch 3 Batch 0 Loss 1.4039 Accuracy 0.4112\n",
            "Epoch 3 Batch 50 Loss 1.4669 Accuracy 0.4053\n",
            "Epoch 3 Batch 100 Loss 1.4348 Accuracy 0.4113\n",
            "Epoch 3 Batch 150 Loss 1.4336 Accuracy 0.4125\n",
            "Epoch 3 Batch 200 Loss 1.4316 Accuracy 0.4128\n",
            "Epoch 3 Batch 250 Loss 1.4304 Accuracy 0.4139\n",
            "Epoch 3 Batch 300 Loss 1.4273 Accuracy 0.4145\n",
            "Epoch 3 Batch 350 Loss 1.4245 Accuracy 0.4143\n",
            "Epoch 3 Batch 400 Loss 1.4236 Accuracy 0.4149\n",
            "Epoch 3 Batch 450 Loss 1.4176 Accuracy 0.4153\n",
            "Epoch 3 Batch 500 Loss 1.4129 Accuracy 0.4155\n",
            "Epoch 3 Batch 550 Loss 1.4127 Accuracy 0.4156\n",
            "Epoch 3 Batch 600 Loss 1.4088 Accuracy 0.4157\n",
            "Epoch 3 Batch 650 Loss 1.4082 Accuracy 0.4161\n",
            "Epoch 3 Batch 700 Loss 1.4041 Accuracy 0.4164\n",
            "Epoch 3 Batch 750 Loss 1.4009 Accuracy 0.4168\n",
            "Epoch 3 Batch 800 Loss 1.3962 Accuracy 0.4171\n",
            "Epoch 3 Batch 850 Loss 1.3904 Accuracy 0.4178\n",
            "Epoch 3 Batch 900 Loss 1.3855 Accuracy 0.4183\n",
            "Epoch 3 Batch 950 Loss 1.3803 Accuracy 0.4186\n",
            "Epoch 3 Batch 1000 Loss 1.3750 Accuracy 0.4193\n",
            "Epoch 3 Batch 1050 Loss 1.3695 Accuracy 0.4202\n",
            "Epoch 3 Batch 1100 Loss 1.3639 Accuracy 0.4206\n",
            "Epoch 3 Batch 1150 Loss 1.3596 Accuracy 0.4211\n",
            "Epoch 3 Batch 1200 Loss 1.3547 Accuracy 0.4217\n",
            "Epoch 3 Batch 1250 Loss 1.3501 Accuracy 0.4225\n",
            "Epoch 3 Batch 1300 Loss 1.3447 Accuracy 0.4231\n",
            "Epoch 3 Batch 1350 Loss 1.3412 Accuracy 0.4240\n",
            "Epoch 3 Batch 1400 Loss 1.3367 Accuracy 0.4247\n",
            "Epoch 3 Batch 1450 Loss 1.3327 Accuracy 0.4254\n",
            "Epoch 3 Batch 1500 Loss 1.3281 Accuracy 0.4261\n",
            "Epoch 3 Batch 1550 Loss 1.3247 Accuracy 0.4268\n",
            "Epoch 3 Batch 1600 Loss 1.3210 Accuracy 0.4275\n",
            "Epoch 3 Batch 1650 Loss 1.3179 Accuracy 0.4282\n",
            "Epoch 3 Batch 1700 Loss 1.3139 Accuracy 0.4287\n",
            "Epoch 3 Batch 1750 Loss 1.3108 Accuracy 0.4292\n",
            "Epoch 3 Batch 1800 Loss 1.3081 Accuracy 0.4300\n",
            "Epoch 3 Batch 1850 Loss 1.3050 Accuracy 0.4305\n",
            "Epoch 3 Batch 1900 Loss 1.3020 Accuracy 0.4312\n",
            "Epoch 3 Batch 1950 Loss 1.3000 Accuracy 0.4317\n",
            "Epoch 3 Batch 2000 Loss 1.2973 Accuracy 0.4321\n",
            "Epoch 3 Batch 2050 Loss 1.2939 Accuracy 0.4326\n",
            "Epoch 3 Batch 2100 Loss 1.2913 Accuracy 0.4332\n",
            "Epoch 3 Batch 2150 Loss 1.2892 Accuracy 0.4336\n",
            "Epoch 3 Batch 2200 Loss 1.2855 Accuracy 0.4341\n",
            "Epoch 3 Batch 2250 Loss 1.2820 Accuracy 0.4344\n",
            "Epoch 3 Batch 2300 Loss 1.2781 Accuracy 0.4348\n",
            "Epoch 3 Batch 2350 Loss 1.2740 Accuracy 0.4352\n",
            "Epoch 3 Batch 2400 Loss 1.2700 Accuracy 0.4354\n",
            "Epoch 3 Batch 2450 Loss 1.2658 Accuracy 0.4355\n",
            "Epoch 3 Batch 2500 Loss 1.2621 Accuracy 0.4358\n",
            "Epoch 3 Batch 2550 Loss 1.2588 Accuracy 0.4362\n",
            "Epoch 3 Batch 2600 Loss 1.2555 Accuracy 0.4365\n",
            "Epoch 3 Batch 2650 Loss 1.2523 Accuracy 0.4368\n",
            "Epoch 3 Batch 2700 Loss 1.2491 Accuracy 0.4370\n",
            "Epoch 3 Batch 2750 Loss 1.2462 Accuracy 0.4374\n",
            "Epoch 3 Batch 2800 Loss 1.2432 Accuracy 0.4377\n",
            "Epoch 3 Batch 2850 Loss 1.2404 Accuracy 0.4381\n",
            "Epoch 3 Batch 2900 Loss 1.2377 Accuracy 0.4385\n",
            "Epoch 3 Batch 2950 Loss 1.2346 Accuracy 0.4389\n",
            "Epoch 3 Batch 3000 Loss 1.2319 Accuracy 0.4393\n",
            "Epoch 3 Batch 3050 Loss 1.2293 Accuracy 0.4397\n",
            "Epoch 3 Batch 3100 Loss 1.2263 Accuracy 0.4401\n",
            "Epoch 3 Batch 3150 Loss 1.2242 Accuracy 0.4405\n",
            "Epoch 3 Batch 3200 Loss 1.2215 Accuracy 0.4407\n",
            "Epoch 3 Batch 3250 Loss 1.2193 Accuracy 0.4411\n",
            "Epoch 3 Batch 3300 Loss 1.2168 Accuracy 0.4415\n",
            "Epoch 3 Batch 3350 Loss 1.2144 Accuracy 0.4419\n",
            "Epoch 3 Batch 3400 Loss 1.2119 Accuracy 0.4423\n",
            "Epoch 3 Batch 3450 Loss 1.2099 Accuracy 0.4427\n",
            "Epoch 3 Batch 3500 Loss 1.2074 Accuracy 0.4430\n",
            "Epoch 3 Batch 3550 Loss 1.2052 Accuracy 0.4434\n",
            "Epoch 3 Batch 3600 Loss 1.2034 Accuracy 0.4437\n",
            "Epoch 3 Batch 3650 Loss 1.2012 Accuracy 0.4440\n",
            "Epoch 3 Batch 3700 Loss 1.1991 Accuracy 0.4443\n",
            "Epoch 3 Batch 3750 Loss 1.1972 Accuracy 0.4448\n",
            "Epoch 3 Batch 3800 Loss 1.1953 Accuracy 0.4451\n",
            "Epoch 3 Batch 3850 Loss 1.1934 Accuracy 0.4455\n",
            "Epoch 3 Batch 3900 Loss 1.1912 Accuracy 0.4459\n",
            "Epoch 3 Batch 3950 Loss 1.1891 Accuracy 0.4462\n",
            "Epoch 3 Batch 4000 Loss 1.1868 Accuracy 0.4466\n",
            "Epoch 3 Batch 4050 Loss 1.1847 Accuracy 0.4471\n",
            "Epoch 3 Batch 4100 Loss 1.1827 Accuracy 0.4475\n",
            "Epoch 3 Batch 4150 Loss 1.1812 Accuracy 0.4479\n",
            "Epoch 3 Batch 4200 Loss 1.1794 Accuracy 0.4482\n",
            "Epoch 3 Batch 4250 Loss 1.1777 Accuracy 0.4486\n",
            "Epoch 3 Batch 4300 Loss 1.1757 Accuracy 0.4490\n",
            "Epoch 3 Batch 4350 Loss 1.1740 Accuracy 0.4494\n",
            "Epoch 3 Batch 4400 Loss 1.1723 Accuracy 0.4497\n",
            "Epoch 3 Batch 4450 Loss 1.1709 Accuracy 0.4500\n",
            "Epoch 3 Batch 4500 Loss 1.1694 Accuracy 0.4503\n",
            "Epoch 3 Batch 4550 Loss 1.1688 Accuracy 0.4504\n",
            "Epoch 3 Batch 4600 Loss 1.1689 Accuracy 0.4504\n",
            "Epoch 3 Batch 4650 Loss 1.1693 Accuracy 0.4504\n",
            "Epoch 3 Batch 4700 Loss 1.1700 Accuracy 0.4504\n",
            "Epoch 3 Batch 4750 Loss 1.1708 Accuracy 0.4503\n",
            "Epoch 3 Batch 4800 Loss 1.1719 Accuracy 0.4502\n",
            "Epoch 3 Batch 4850 Loss 1.1732 Accuracy 0.4500\n",
            "Epoch 3 Batch 4900 Loss 1.1746 Accuracy 0.4498\n",
            "Epoch 3 Batch 4950 Loss 1.1758 Accuracy 0.4497\n",
            "Epoch 3 Batch 5000 Loss 1.1774 Accuracy 0.4494\n",
            "Epoch 3 Batch 5050 Loss 1.1789 Accuracy 0.4492\n",
            "Epoch 3 Batch 5100 Loss 1.1808 Accuracy 0.4489\n",
            "Epoch 3 Batch 5150 Loss 1.1824 Accuracy 0.4487\n",
            "Epoch 3 Batch 5200 Loss 1.1840 Accuracy 0.4486\n",
            "Epoch 3 Batch 5250 Loss 1.1857 Accuracy 0.4483\n",
            "Epoch 3 Batch 5300 Loss 1.1871 Accuracy 0.4482\n",
            "Epoch 3 Batch 5350 Loss 1.1884 Accuracy 0.4481\n",
            "Epoch 3 Batch 5400 Loss 1.1900 Accuracy 0.4479\n",
            "Epoch 3 Batch 5450 Loss 1.1913 Accuracy 0.4477\n",
            "Epoch 3 Batch 5500 Loss 1.1928 Accuracy 0.4475\n",
            "Epoch 3 Batch 5550 Loss 1.1943 Accuracy 0.4473\n",
            "Epoch 3 Batch 5600 Loss 1.1958 Accuracy 0.4471\n",
            "Epoch 3 Batch 5650 Loss 1.1972 Accuracy 0.4468\n",
            "Epoch 3 Batch 5700 Loss 1.1986 Accuracy 0.4466\n",
            "Epoch 3 Batch 5750 Loss 1.1998 Accuracy 0.4463\n",
            "Epoch 3 Batch 5800 Loss 1.2009 Accuracy 0.4461\n",
            "Epoch 3 Batch 5850 Loss 1.2022 Accuracy 0.4459\n",
            "Epoch 3 Batch 5900 Loss 1.2035 Accuracy 0.4457\n",
            "Epoch 3 Batch 5950 Loss 1.2044 Accuracy 0.4455\n",
            "Epoch 3 Batch 6000 Loss 1.2054 Accuracy 0.4453\n",
            "Epoch 3 Batch 6050 Loss 1.2064 Accuracy 0.4451\n",
            "Epoch 3 Batch 6100 Loss 1.2077 Accuracy 0.4449\n",
            "Epoch 3 Batch 6150 Loss 1.2085 Accuracy 0.4447\n",
            "Saving checkpoint for epoch 3 at ./drive/My Drive/nlp/ckpt/ckpt-4\n",
            "Time taken for 1 epoch: 1779.6005411148071 secs\n",
            "\n",
            "Start of epoch 4\n",
            "Epoch 4 Batch 0 Loss 1.3873 Accuracy 0.4145\n",
            "Epoch 4 Batch 50 Loss 1.3492 Accuracy 0.4255\n",
            "Epoch 4 Batch 100 Loss 1.3422 Accuracy 0.4274\n",
            "Epoch 4 Batch 150 Loss 1.3397 Accuracy 0.4270\n",
            "Epoch 4 Batch 200 Loss 1.3419 Accuracy 0.4279\n",
            "Epoch 4 Batch 250 Loss 1.3393 Accuracy 0.4287\n",
            "Epoch 4 Batch 300 Loss 1.3360 Accuracy 0.4295\n",
            "Epoch 4 Batch 350 Loss 1.3326 Accuracy 0.4297\n",
            "Epoch 4 Batch 400 Loss 1.3251 Accuracy 0.4300\n",
            "Epoch 4 Batch 450 Loss 1.3227 Accuracy 0.4298\n",
            "Epoch 4 Batch 500 Loss 1.3179 Accuracy 0.4295\n",
            "Epoch 4 Batch 550 Loss 1.3170 Accuracy 0.4295\n",
            "Epoch 4 Batch 600 Loss 1.3150 Accuracy 0.4294\n",
            "Epoch 4 Batch 650 Loss 1.3117 Accuracy 0.4293\n",
            "Epoch 4 Batch 700 Loss 1.3075 Accuracy 0.4301\n",
            "Epoch 4 Batch 750 Loss 1.3036 Accuracy 0.4307\n",
            "Epoch 4 Batch 800 Loss 1.3005 Accuracy 0.4314\n",
            "Epoch 4 Batch 850 Loss 1.2970 Accuracy 0.4318\n",
            "Epoch 4 Batch 900 Loss 1.2915 Accuracy 0.4322\n",
            "Epoch 4 Batch 950 Loss 1.2863 Accuracy 0.4328\n",
            "Epoch 4 Batch 1000 Loss 1.2809 Accuracy 0.4334\n",
            "Epoch 4 Batch 1050 Loss 1.2758 Accuracy 0.4335\n",
            "Epoch 4 Batch 1100 Loss 1.2711 Accuracy 0.4341\n",
            "Epoch 4 Batch 1150 Loss 1.2659 Accuracy 0.4347\n",
            "Epoch 4 Batch 1200 Loss 1.2620 Accuracy 0.4354\n",
            "Epoch 4 Batch 1250 Loss 1.2572 Accuracy 0.4361\n",
            "Epoch 4 Batch 1300 Loss 1.2529 Accuracy 0.4369\n",
            "Epoch 4 Batch 1350 Loss 1.2485 Accuracy 0.4377\n",
            "Epoch 4 Batch 1400 Loss 1.2446 Accuracy 0.4382\n",
            "Epoch 4 Batch 1450 Loss 1.2410 Accuracy 0.4388\n",
            "Epoch 4 Batch 1500 Loss 1.2375 Accuracy 0.4395\n",
            "Epoch 4 Batch 1550 Loss 1.2332 Accuracy 0.4402\n",
            "Epoch 4 Batch 1600 Loss 1.2294 Accuracy 0.4408\n",
            "Epoch 4 Batch 1650 Loss 1.2248 Accuracy 0.4415\n",
            "Epoch 4 Batch 1700 Loss 1.2222 Accuracy 0.4422\n",
            "Epoch 4 Batch 1750 Loss 1.2193 Accuracy 0.4429\n",
            "Epoch 4 Batch 1800 Loss 1.2171 Accuracy 0.4436\n",
            "Epoch 4 Batch 1850 Loss 1.2143 Accuracy 0.4441\n",
            "Epoch 4 Batch 1900 Loss 1.2115 Accuracy 0.4447\n",
            "Epoch 4 Batch 1950 Loss 1.2085 Accuracy 0.4451\n",
            "Epoch 4 Batch 2000 Loss 1.2058 Accuracy 0.4456\n",
            "Epoch 4 Batch 2050 Loss 1.2026 Accuracy 0.4462\n",
            "Epoch 4 Batch 2100 Loss 1.1996 Accuracy 0.4467\n",
            "Epoch 4 Batch 2150 Loss 1.1964 Accuracy 0.4473\n",
            "Epoch 4 Batch 2200 Loss 1.1938 Accuracy 0.4476\n",
            "Epoch 4 Batch 2250 Loss 1.1913 Accuracy 0.4481\n",
            "Epoch 4 Batch 2300 Loss 1.1878 Accuracy 0.4484\n",
            "Epoch 4 Batch 2350 Loss 1.1836 Accuracy 0.4486\n",
            "Epoch 4 Batch 2400 Loss 1.1797 Accuracy 0.4489\n",
            "Epoch 4 Batch 2450 Loss 1.1762 Accuracy 0.4491\n",
            "Epoch 4 Batch 2500 Loss 1.1730 Accuracy 0.4494\n",
            "Epoch 4 Batch 2550 Loss 1.1693 Accuracy 0.4496\n",
            "Epoch 4 Batch 2600 Loss 1.1660 Accuracy 0.4498\n",
            "Epoch 4 Batch 2650 Loss 1.1630 Accuracy 0.4500\n",
            "Epoch 4 Batch 2700 Loss 1.1604 Accuracy 0.4502\n",
            "Epoch 4 Batch 2750 Loss 1.1577 Accuracy 0.4504\n",
            "Epoch 4 Batch 2800 Loss 1.1553 Accuracy 0.4508\n",
            "Epoch 4 Batch 2850 Loss 1.1527 Accuracy 0.4511\n",
            "Epoch 4 Batch 2900 Loss 1.1504 Accuracy 0.4515\n",
            "Epoch 4 Batch 2950 Loss 1.1482 Accuracy 0.4519\n",
            "Epoch 4 Batch 3000 Loss 1.1455 Accuracy 0.4523\n",
            "Epoch 4 Batch 3050 Loss 1.1435 Accuracy 0.4526\n",
            "Epoch 4 Batch 3100 Loss 1.1413 Accuracy 0.4528\n",
            "Epoch 4 Batch 3150 Loss 1.1394 Accuracy 0.4531\n",
            "Epoch 4 Batch 3200 Loss 1.1372 Accuracy 0.4535\n",
            "Epoch 4 Batch 3250 Loss 1.1351 Accuracy 0.4538\n",
            "Epoch 4 Batch 3300 Loss 1.1330 Accuracy 0.4542\n",
            "Epoch 4 Batch 3350 Loss 1.1311 Accuracy 0.4546\n",
            "Epoch 4 Batch 3400 Loss 1.1291 Accuracy 0.4549\n",
            "Epoch 4 Batch 3450 Loss 1.1271 Accuracy 0.4553\n",
            "Epoch 4 Batch 3500 Loss 1.1250 Accuracy 0.4556\n",
            "Epoch 4 Batch 3550 Loss 1.1230 Accuracy 0.4558\n",
            "Epoch 4 Batch 3600 Loss 1.1208 Accuracy 0.4562\n",
            "Epoch 4 Batch 3650 Loss 1.1188 Accuracy 0.4565\n",
            "Epoch 4 Batch 3700 Loss 1.1167 Accuracy 0.4569\n",
            "Epoch 4 Batch 3750 Loss 1.1146 Accuracy 0.4572\n",
            "Epoch 4 Batch 3800 Loss 1.1132 Accuracy 0.4575\n",
            "Epoch 4 Batch 3850 Loss 1.1113 Accuracy 0.4578\n",
            "Epoch 4 Batch 3900 Loss 1.1098 Accuracy 0.4582\n",
            "Epoch 4 Batch 3950 Loss 1.1078 Accuracy 0.4586\n",
            "Epoch 4 Batch 4000 Loss 1.1057 Accuracy 0.4590\n",
            "Epoch 4 Batch 4050 Loss 1.1035 Accuracy 0.4594\n",
            "Epoch 4 Batch 4100 Loss 1.1016 Accuracy 0.4597\n",
            "Epoch 4 Batch 4150 Loss 1.1001 Accuracy 0.4600\n",
            "Epoch 4 Batch 4200 Loss 1.0983 Accuracy 0.4604\n",
            "Epoch 4 Batch 4250 Loss 1.0967 Accuracy 0.4608\n",
            "Epoch 4 Batch 4300 Loss 1.0952 Accuracy 0.4612\n",
            "Epoch 4 Batch 4350 Loss 1.0938 Accuracy 0.4615\n",
            "Epoch 4 Batch 4400 Loss 1.0923 Accuracy 0.4617\n",
            "Epoch 4 Batch 4450 Loss 1.0909 Accuracy 0.4621\n",
            "Epoch 4 Batch 4500 Loss 1.0900 Accuracy 0.4623\n",
            "Epoch 4 Batch 4550 Loss 1.0894 Accuracy 0.4624\n",
            "Epoch 4 Batch 4600 Loss 1.0895 Accuracy 0.4625\n",
            "Epoch 4 Batch 4650 Loss 1.0900 Accuracy 0.4624\n",
            "Epoch 4 Batch 4700 Loss 1.0909 Accuracy 0.4624\n",
            "Epoch 4 Batch 4750 Loss 1.0921 Accuracy 0.4623\n",
            "Epoch 4 Batch 4800 Loss 1.0935 Accuracy 0.4621\n",
            "Epoch 4 Batch 4850 Loss 1.0950 Accuracy 0.4619\n",
            "Epoch 4 Batch 4900 Loss 1.0964 Accuracy 0.4617\n",
            "Epoch 4 Batch 4950 Loss 1.0978 Accuracy 0.4615\n",
            "Epoch 4 Batch 5000 Loss 1.0996 Accuracy 0.4612\n",
            "Epoch 4 Batch 5050 Loss 1.1014 Accuracy 0.4609\n",
            "Epoch 4 Batch 5100 Loss 1.1031 Accuracy 0.4607\n",
            "Epoch 4 Batch 5150 Loss 1.1047 Accuracy 0.4605\n",
            "Epoch 4 Batch 5200 Loss 1.1063 Accuracy 0.4603\n",
            "Epoch 4 Batch 5250 Loss 1.1078 Accuracy 0.4600\n",
            "Epoch 4 Batch 5300 Loss 1.1096 Accuracy 0.4598\n",
            "Epoch 4 Batch 5350 Loss 1.1113 Accuracy 0.4596\n",
            "Epoch 4 Batch 5400 Loss 1.1129 Accuracy 0.4594\n",
            "Epoch 4 Batch 5450 Loss 1.1143 Accuracy 0.4592\n",
            "Epoch 4 Batch 5500 Loss 1.1160 Accuracy 0.4590\n",
            "Epoch 4 Batch 5550 Loss 1.1174 Accuracy 0.4587\n",
            "Epoch 4 Batch 5600 Loss 1.1190 Accuracy 0.4585\n",
            "Epoch 4 Batch 5650 Loss 1.1205 Accuracy 0.4582\n",
            "Epoch 4 Batch 5700 Loss 1.1222 Accuracy 0.4580\n",
            "Epoch 4 Batch 5750 Loss 1.1236 Accuracy 0.4577\n",
            "Epoch 4 Batch 5800 Loss 1.1252 Accuracy 0.4575\n",
            "Epoch 4 Batch 5850 Loss 1.1268 Accuracy 0.4572\n",
            "Epoch 4 Batch 5900 Loss 1.1283 Accuracy 0.4570\n",
            "Epoch 4 Batch 5950 Loss 1.1295 Accuracy 0.4567\n",
            "Epoch 4 Batch 6000 Loss 1.1305 Accuracy 0.4565\n",
            "Epoch 4 Batch 6050 Loss 1.1317 Accuracy 0.4563\n",
            "Epoch 4 Batch 6100 Loss 1.1328 Accuracy 0.4561\n",
            "Epoch 4 Batch 6150 Loss 1.1338 Accuracy 0.4559\n",
            "Saving checkpoint for epoch 4 at ./drive/My Drive/nlp/ckpt/ckpt-5\n",
            "Time taken for 1 epoch: 1782.9971988201141 secs\n",
            "\n",
            "Start of epoch 5\n",
            "Epoch 5 Batch 0 Loss 1.4624 Accuracy 0.4359\n",
            "Epoch 5 Batch 50 Loss 1.3130 Accuracy 0.4333\n",
            "Epoch 5 Batch 100 Loss 1.3052 Accuracy 0.4378\n",
            "Epoch 5 Batch 150 Loss 1.2967 Accuracy 0.4367\n",
            "Epoch 5 Batch 200 Loss 1.2894 Accuracy 0.4367\n",
            "Epoch 5 Batch 250 Loss 1.2810 Accuracy 0.4370\n",
            "Epoch 5 Batch 300 Loss 1.2771 Accuracy 0.4372\n",
            "Epoch 5 Batch 350 Loss 1.2732 Accuracy 0.4372\n",
            "Epoch 5 Batch 400 Loss 1.2715 Accuracy 0.4375\n",
            "Epoch 5 Batch 450 Loss 1.2673 Accuracy 0.4380\n",
            "Epoch 5 Batch 500 Loss 1.2642 Accuracy 0.4384\n",
            "Epoch 5 Batch 550 Loss 1.2621 Accuracy 0.4382\n",
            "Epoch 5 Batch 600 Loss 1.2604 Accuracy 0.4383\n",
            "Epoch 5 Batch 650 Loss 1.2601 Accuracy 0.4379\n",
            "Epoch 5 Batch 700 Loss 1.2573 Accuracy 0.4380\n",
            "Epoch 5 Batch 750 Loss 1.2511 Accuracy 0.4389\n",
            "Epoch 5 Batch 800 Loss 1.2462 Accuracy 0.4395\n",
            "Epoch 5 Batch 850 Loss 1.2403 Accuracy 0.4400\n",
            "Epoch 5 Batch 900 Loss 1.2350 Accuracy 0.4407\n",
            "Epoch 5 Batch 950 Loss 1.2298 Accuracy 0.4412\n",
            "Epoch 5 Batch 1000 Loss 1.2238 Accuracy 0.4417\n",
            "Epoch 5 Batch 1050 Loss 1.2184 Accuracy 0.4421\n",
            "Epoch 5 Batch 1100 Loss 1.2144 Accuracy 0.4431\n",
            "Epoch 5 Batch 1150 Loss 1.2099 Accuracy 0.4436\n",
            "Epoch 5 Batch 1200 Loss 1.2047 Accuracy 0.4442\n",
            "Epoch 5 Batch 1250 Loss 1.2013 Accuracy 0.4450\n",
            "Epoch 5 Batch 1300 Loss 1.1968 Accuracy 0.4456\n",
            "Epoch 5 Batch 1350 Loss 1.1933 Accuracy 0.4463\n",
            "Epoch 5 Batch 1400 Loss 1.1889 Accuracy 0.4471\n",
            "Epoch 5 Batch 1450 Loss 1.1852 Accuracy 0.4478\n",
            "Epoch 5 Batch 1500 Loss 1.1818 Accuracy 0.4483\n",
            "Epoch 5 Batch 1550 Loss 1.1783 Accuracy 0.4489\n",
            "Epoch 5 Batch 1600 Loss 1.1745 Accuracy 0.4496\n",
            "Epoch 5 Batch 1650 Loss 1.1709 Accuracy 0.4503\n",
            "Epoch 5 Batch 1700 Loss 1.1664 Accuracy 0.4508\n",
            "Epoch 5 Batch 1750 Loss 1.1631 Accuracy 0.4514\n",
            "Epoch 5 Batch 1800 Loss 1.1617 Accuracy 0.4522\n",
            "Epoch 5 Batch 1850 Loss 1.1589 Accuracy 0.4528\n",
            "Epoch 5 Batch 1900 Loss 1.1561 Accuracy 0.4533\n",
            "Epoch 5 Batch 1950 Loss 1.1535 Accuracy 0.4537\n",
            "Epoch 5 Batch 2000 Loss 1.1507 Accuracy 0.4543\n",
            "Epoch 5 Batch 2050 Loss 1.1479 Accuracy 0.4547\n",
            "Epoch 5 Batch 2100 Loss 1.1449 Accuracy 0.4552\n",
            "Epoch 5 Batch 2150 Loss 1.1422 Accuracy 0.4556\n",
            "Epoch 5 Batch 2200 Loss 1.1394 Accuracy 0.4560\n",
            "Epoch 5 Batch 2250 Loss 1.1367 Accuracy 0.4563\n",
            "Epoch 5 Batch 2300 Loss 1.1339 Accuracy 0.4565\n",
            "Epoch 5 Batch 2350 Loss 1.1303 Accuracy 0.4568\n",
            "Epoch 5 Batch 2400 Loss 1.1273 Accuracy 0.4571\n",
            "Epoch 5 Batch 2450 Loss 1.1235 Accuracy 0.4573\n",
            "Epoch 5 Batch 2500 Loss 1.1204 Accuracy 0.4575\n",
            "Epoch 5 Batch 2550 Loss 1.1177 Accuracy 0.4577\n",
            "Epoch 5 Batch 2600 Loss 1.1139 Accuracy 0.4579\n",
            "Epoch 5 Batch 2650 Loss 1.1110 Accuracy 0.4582\n",
            "Epoch 5 Batch 2700 Loss 1.1084 Accuracy 0.4585\n",
            "Epoch 5 Batch 2750 Loss 1.1056 Accuracy 0.4587\n",
            "Epoch 5 Batch 2800 Loss 1.1033 Accuracy 0.4590\n",
            "Epoch 5 Batch 2850 Loss 1.1009 Accuracy 0.4593\n",
            "Epoch 5 Batch 2900 Loss 1.0982 Accuracy 0.4596\n",
            "Epoch 5 Batch 2950 Loss 1.0960 Accuracy 0.4600\n",
            "Epoch 5 Batch 3000 Loss 1.0934 Accuracy 0.4602\n",
            "Epoch 5 Batch 3050 Loss 1.0911 Accuracy 0.4605\n",
            "Epoch 5 Batch 3100 Loss 1.0890 Accuracy 0.4608\n",
            "Epoch 5 Batch 3150 Loss 1.0870 Accuracy 0.4612\n",
            "Epoch 5 Batch 3200 Loss 1.0852 Accuracy 0.4615\n",
            "Epoch 5 Batch 3250 Loss 1.0832 Accuracy 0.4618\n",
            "Epoch 5 Batch 3300 Loss 1.0815 Accuracy 0.4622\n",
            "Epoch 5 Batch 3350 Loss 1.0795 Accuracy 0.4625\n",
            "Epoch 5 Batch 3400 Loss 1.0773 Accuracy 0.4628\n",
            "Epoch 5 Batch 3450 Loss 1.0753 Accuracy 0.4631\n",
            "Epoch 5 Batch 3500 Loss 1.0731 Accuracy 0.4634\n",
            "Epoch 5 Batch 3550 Loss 1.0714 Accuracy 0.4637\n",
            "Epoch 5 Batch 3600 Loss 1.0694 Accuracy 0.4640\n",
            "Epoch 5 Batch 3650 Loss 1.0674 Accuracy 0.4643\n",
            "Epoch 5 Batch 3700 Loss 1.0655 Accuracy 0.4647\n",
            "Epoch 5 Batch 3750 Loss 1.0639 Accuracy 0.4650\n",
            "Epoch 5 Batch 3800 Loss 1.0623 Accuracy 0.4653\n",
            "Epoch 5 Batch 3850 Loss 1.0606 Accuracy 0.4656\n",
            "Epoch 5 Batch 3900 Loss 1.0588 Accuracy 0.4660\n",
            "Epoch 5 Batch 3950 Loss 1.0572 Accuracy 0.4663\n",
            "Epoch 5 Batch 4000 Loss 1.0554 Accuracy 0.4667\n",
            "Epoch 5 Batch 4050 Loss 1.0537 Accuracy 0.4670\n",
            "Epoch 5 Batch 4100 Loss 1.0519 Accuracy 0.4674\n",
            "Epoch 5 Batch 4150 Loss 1.0504 Accuracy 0.4677\n",
            "Epoch 5 Batch 4200 Loss 1.0489 Accuracy 0.4681\n",
            "Epoch 5 Batch 4250 Loss 1.0474 Accuracy 0.4685\n",
            "Epoch 5 Batch 4300 Loss 1.0458 Accuracy 0.4688\n",
            "Epoch 5 Batch 4350 Loss 1.0446 Accuracy 0.4691\n",
            "Epoch 5 Batch 4400 Loss 1.0431 Accuracy 0.4695\n",
            "Epoch 5 Batch 4450 Loss 1.0419 Accuracy 0.4698\n",
            "Epoch 5 Batch 4500 Loss 1.0408 Accuracy 0.4701\n",
            "Epoch 5 Batch 4550 Loss 1.0407 Accuracy 0.4702\n",
            "Epoch 5 Batch 4600 Loss 1.0410 Accuracy 0.4702\n",
            "Epoch 5 Batch 4650 Loss 1.0416 Accuracy 0.4702\n",
            "Epoch 5 Batch 4700 Loss 1.0422 Accuracy 0.4702\n",
            "Epoch 5 Batch 4750 Loss 1.0432 Accuracy 0.4701\n",
            "Epoch 5 Batch 4800 Loss 1.0446 Accuracy 0.4699\n",
            "Epoch 5 Batch 4850 Loss 1.0459 Accuracy 0.4697\n",
            "Epoch 5 Batch 4900 Loss 1.0475 Accuracy 0.4694\n",
            "Epoch 5 Batch 4950 Loss 1.0491 Accuracy 0.4691\n",
            "Epoch 5 Batch 5000 Loss 1.0504 Accuracy 0.4689\n",
            "Epoch 5 Batch 5050 Loss 1.0523 Accuracy 0.4686\n",
            "Epoch 5 Batch 5100 Loss 1.0540 Accuracy 0.4683\n",
            "Epoch 5 Batch 5150 Loss 1.0556 Accuracy 0.4680\n",
            "Epoch 5 Batch 5200 Loss 1.0574 Accuracy 0.4678\n",
            "Epoch 5 Batch 5250 Loss 1.0592 Accuracy 0.4676\n",
            "Epoch 5 Batch 5300 Loss 1.0609 Accuracy 0.4674\n",
            "Epoch 5 Batch 5350 Loss 1.0623 Accuracy 0.4671\n",
            "Epoch 5 Batch 5400 Loss 1.0639 Accuracy 0.4669\n",
            "Epoch 5 Batch 5450 Loss 1.0656 Accuracy 0.4666\n",
            "Epoch 5 Batch 5500 Loss 1.0671 Accuracy 0.4664\n",
            "Epoch 5 Batch 5550 Loss 1.0690 Accuracy 0.4661\n",
            "Epoch 5 Batch 5600 Loss 1.0708 Accuracy 0.4659\n",
            "Epoch 5 Batch 5650 Loss 1.0726 Accuracy 0.4656\n",
            "Epoch 5 Batch 5700 Loss 1.0742 Accuracy 0.4653\n",
            "Epoch 5 Batch 5750 Loss 1.0758 Accuracy 0.4650\n",
            "Epoch 5 Batch 5800 Loss 1.0773 Accuracy 0.4647\n",
            "Epoch 5 Batch 5850 Loss 1.0790 Accuracy 0.4645\n",
            "Epoch 5 Batch 5900 Loss 1.0803 Accuracy 0.4642\n",
            "Epoch 5 Batch 5950 Loss 1.0816 Accuracy 0.4640\n",
            "Epoch 5 Batch 6000 Loss 1.0830 Accuracy 0.4638\n",
            "Epoch 5 Batch 6050 Loss 1.0843 Accuracy 0.4636\n",
            "Epoch 5 Batch 6100 Loss 1.0853 Accuracy 0.4633\n",
            "Epoch 5 Batch 6150 Loss 1.0867 Accuracy 0.4631\n",
            "Saving checkpoint for epoch 5 at ./drive/My Drive/nlp/ckpt/ckpt-6\n",
            "Time taken for 1 epoch: 1791.370671749115 secs\n",
            "\n",
            "Start of epoch 6\n",
            "Epoch 6 Batch 0 Loss 1.2570 Accuracy 0.4416\n",
            "Epoch 6 Batch 50 Loss 1.2725 Accuracy 0.4386\n",
            "Epoch 6 Batch 100 Loss 1.2579 Accuracy 0.4422\n",
            "Epoch 6 Batch 150 Loss 1.2543 Accuracy 0.4416\n",
            "Epoch 6 Batch 200 Loss 1.2456 Accuracy 0.4422\n",
            "Epoch 6 Batch 250 Loss 1.2415 Accuracy 0.4428\n",
            "Epoch 6 Batch 300 Loss 1.2356 Accuracy 0.4434\n",
            "Epoch 6 Batch 350 Loss 1.2287 Accuracy 0.4435\n",
            "Epoch 6 Batch 400 Loss 1.2254 Accuracy 0.4440\n",
            "Epoch 6 Batch 450 Loss 1.2251 Accuracy 0.4437\n",
            "Epoch 6 Batch 500 Loss 1.2239 Accuracy 0.4435\n",
            "Epoch 6 Batch 550 Loss 1.2211 Accuracy 0.4435\n",
            "Epoch 6 Batch 600 Loss 1.2216 Accuracy 0.4432\n",
            "Epoch 6 Batch 650 Loss 1.2187 Accuracy 0.4433\n",
            "Epoch 6 Batch 700 Loss 1.2149 Accuracy 0.4441\n",
            "Epoch 6 Batch 750 Loss 1.2120 Accuracy 0.4446\n",
            "Epoch 6 Batch 800 Loss 1.2082 Accuracy 0.4449\n",
            "Epoch 6 Batch 850 Loss 1.2029 Accuracy 0.4456\n",
            "Epoch 6 Batch 900 Loss 1.1994 Accuracy 0.4462\n",
            "Epoch 6 Batch 950 Loss 1.1943 Accuracy 0.4468\n",
            "Epoch 6 Batch 1000 Loss 1.1881 Accuracy 0.4475\n",
            "Epoch 6 Batch 1050 Loss 1.1831 Accuracy 0.4480\n",
            "Epoch 6 Batch 1100 Loss 1.1768 Accuracy 0.4487\n",
            "Epoch 6 Batch 1150 Loss 1.1717 Accuracy 0.4492\n",
            "Epoch 6 Batch 1200 Loss 1.1677 Accuracy 0.4499\n",
            "Epoch 6 Batch 1250 Loss 1.1635 Accuracy 0.4507\n",
            "Epoch 6 Batch 1300 Loss 1.1594 Accuracy 0.4514\n",
            "Epoch 6 Batch 1350 Loss 1.1552 Accuracy 0.4519\n",
            "Epoch 6 Batch 1400 Loss 1.1516 Accuracy 0.4527\n",
            "Epoch 6 Batch 1450 Loss 1.1482 Accuracy 0.4534\n",
            "Epoch 6 Batch 1500 Loss 1.1441 Accuracy 0.4541\n",
            "Epoch 6 Batch 1550 Loss 1.1405 Accuracy 0.4546\n",
            "Epoch 6 Batch 1600 Loss 1.1372 Accuracy 0.4553\n",
            "Epoch 6 Batch 1650 Loss 1.1332 Accuracy 0.4560\n",
            "Epoch 6 Batch 1700 Loss 1.1302 Accuracy 0.4567\n",
            "Epoch 6 Batch 1750 Loss 1.1271 Accuracy 0.4574\n",
            "Epoch 6 Batch 1800 Loss 1.1246 Accuracy 0.4580\n",
            "Epoch 6 Batch 1850 Loss 1.1221 Accuracy 0.4585\n",
            "Epoch 6 Batch 1900 Loss 1.1192 Accuracy 0.4590\n",
            "Epoch 6 Batch 1950 Loss 1.1165 Accuracy 0.4595\n",
            "Epoch 6 Batch 2000 Loss 1.1138 Accuracy 0.4599\n",
            "Epoch 6 Batch 2050 Loss 1.1109 Accuracy 0.4605\n",
            "Epoch 6 Batch 2100 Loss 1.1078 Accuracy 0.4609\n",
            "Epoch 6 Batch 2150 Loss 1.1049 Accuracy 0.4614\n",
            "Epoch 6 Batch 2200 Loss 1.1017 Accuracy 0.4617\n",
            "Epoch 6 Batch 2250 Loss 1.0989 Accuracy 0.4619\n",
            "Epoch 6 Batch 2300 Loss 1.0956 Accuracy 0.4622\n",
            "Epoch 6 Batch 2350 Loss 1.0922 Accuracy 0.4624\n",
            "Epoch 6 Batch 2400 Loss 1.0884 Accuracy 0.4626\n",
            "Epoch 6 Batch 2450 Loss 1.0848 Accuracy 0.4627\n",
            "Epoch 6 Batch 2500 Loss 1.0819 Accuracy 0.4630\n",
            "Epoch 6 Batch 2550 Loss 1.0791 Accuracy 0.4632\n",
            "Epoch 6 Batch 2600 Loss 1.0767 Accuracy 0.4635\n",
            "Epoch 6 Batch 2650 Loss 1.0737 Accuracy 0.4637\n",
            "Epoch 6 Batch 2700 Loss 1.0713 Accuracy 0.4639\n",
            "Epoch 6 Batch 2750 Loss 1.0690 Accuracy 0.4642\n",
            "Epoch 6 Batch 2800 Loss 1.0668 Accuracy 0.4645\n",
            "Epoch 6 Batch 2850 Loss 1.0644 Accuracy 0.4647\n",
            "Epoch 6 Batch 2900 Loss 1.0621 Accuracy 0.4650\n",
            "Epoch 6 Batch 2950 Loss 1.0601 Accuracy 0.4653\n",
            "Epoch 6 Batch 3000 Loss 1.0574 Accuracy 0.4657\n",
            "Epoch 6 Batch 3050 Loss 1.0548 Accuracy 0.4660\n",
            "Epoch 6 Batch 3100 Loss 1.0527 Accuracy 0.4664\n",
            "Epoch 6 Batch 3150 Loss 1.0507 Accuracy 0.4667\n",
            "Epoch 6 Batch 3200 Loss 1.0486 Accuracy 0.4670\n",
            "Epoch 6 Batch 3250 Loss 1.0465 Accuracy 0.4673\n",
            "Epoch 6 Batch 3300 Loss 1.0448 Accuracy 0.4677\n",
            "Epoch 6 Batch 3350 Loss 1.0427 Accuracy 0.4680\n",
            "Epoch 6 Batch 3400 Loss 1.0409 Accuracy 0.4685\n",
            "Epoch 6 Batch 3450 Loss 1.0391 Accuracy 0.4688\n",
            "Epoch 6 Batch 3500 Loss 1.0373 Accuracy 0.4690\n",
            "Epoch 6 Batch 3550 Loss 1.0355 Accuracy 0.4693\n",
            "Epoch 6 Batch 3600 Loss 1.0334 Accuracy 0.4697\n",
            "Epoch 6 Batch 3650 Loss 1.0316 Accuracy 0.4699\n",
            "Epoch 6 Batch 3700 Loss 1.0298 Accuracy 0.4702\n",
            "Epoch 6 Batch 3750 Loss 1.0281 Accuracy 0.4705\n",
            "Epoch 6 Batch 3800 Loss 1.0265 Accuracy 0.4709\n",
            "Epoch 6 Batch 3850 Loss 1.0246 Accuracy 0.4712\n",
            "Epoch 6 Batch 3900 Loss 1.0228 Accuracy 0.4716\n",
            "Epoch 6 Batch 3950 Loss 1.0213 Accuracy 0.4719\n",
            "Epoch 6 Batch 4000 Loss 1.0198 Accuracy 0.4723\n",
            "Epoch 6 Batch 4050 Loss 1.0182 Accuracy 0.4726\n",
            "Epoch 6 Batch 4100 Loss 1.0168 Accuracy 0.4730\n",
            "Epoch 6 Batch 4150 Loss 1.0152 Accuracy 0.4732\n",
            "Epoch 6 Batch 4200 Loss 1.0136 Accuracy 0.4736\n",
            "Epoch 6 Batch 4250 Loss 1.0123 Accuracy 0.4739\n",
            "Epoch 6 Batch 4300 Loss 1.0109 Accuracy 0.4743\n",
            "Epoch 6 Batch 4350 Loss 1.0093 Accuracy 0.4747\n",
            "Epoch 6 Batch 4400 Loss 1.0081 Accuracy 0.4750\n",
            "Epoch 6 Batch 4450 Loss 1.0067 Accuracy 0.4753\n",
            "Epoch 6 Batch 4500 Loss 1.0056 Accuracy 0.4756\n",
            "Epoch 6 Batch 4550 Loss 1.0054 Accuracy 0.4756\n",
            "Epoch 6 Batch 4600 Loss 1.0058 Accuracy 0.4756\n",
            "Epoch 6 Batch 4650 Loss 1.0065 Accuracy 0.4756\n",
            "Epoch 6 Batch 4700 Loss 1.0074 Accuracy 0.4755\n",
            "Epoch 6 Batch 4750 Loss 1.0084 Accuracy 0.4754\n",
            "Epoch 6 Batch 4800 Loss 1.0098 Accuracy 0.4752\n",
            "Epoch 6 Batch 4850 Loss 1.0112 Accuracy 0.4750\n",
            "Epoch 6 Batch 4900 Loss 1.0130 Accuracy 0.4748\n",
            "Epoch 6 Batch 4950 Loss 1.0145 Accuracy 0.4744\n",
            "Epoch 6 Batch 5000 Loss 1.0162 Accuracy 0.4742\n",
            "Epoch 6 Batch 5050 Loss 1.0182 Accuracy 0.4740\n",
            "Epoch 6 Batch 5100 Loss 1.0200 Accuracy 0.4737\n",
            "Epoch 6 Batch 5150 Loss 1.0217 Accuracy 0.4734\n",
            "Epoch 6 Batch 5200 Loss 1.0239 Accuracy 0.4731\n",
            "Epoch 6 Batch 5250 Loss 1.0255 Accuracy 0.4729\n",
            "Epoch 6 Batch 5300 Loss 1.0272 Accuracy 0.4727\n",
            "Epoch 6 Batch 5350 Loss 1.0286 Accuracy 0.4725\n",
            "Epoch 6 Batch 5400 Loss 1.0302 Accuracy 0.4723\n",
            "Epoch 6 Batch 5450 Loss 1.0321 Accuracy 0.4720\n",
            "Epoch 6 Batch 5500 Loss 1.0336 Accuracy 0.4718\n",
            "Epoch 6 Batch 5550 Loss 1.0352 Accuracy 0.4715\n",
            "Epoch 6 Batch 5600 Loss 1.0370 Accuracy 0.4713\n",
            "Epoch 6 Batch 5650 Loss 1.0386 Accuracy 0.4710\n",
            "Epoch 6 Batch 5700 Loss 1.0403 Accuracy 0.4707\n",
            "Epoch 6 Batch 5750 Loss 1.0419 Accuracy 0.4705\n",
            "Epoch 6 Batch 5800 Loss 1.0435 Accuracy 0.4702\n",
            "Epoch 6 Batch 5850 Loss 1.0452 Accuracy 0.4699\n",
            "Epoch 6 Batch 5900 Loss 1.0468 Accuracy 0.4696\n",
            "Epoch 6 Batch 5950 Loss 1.0481 Accuracy 0.4694\n",
            "Epoch 6 Batch 6000 Loss 1.0495 Accuracy 0.4692\n",
            "Epoch 6 Batch 6050 Loss 1.0508 Accuracy 0.4689\n",
            "Epoch 6 Batch 6100 Loss 1.0519 Accuracy 0.4687\n",
            "Epoch 6 Batch 6150 Loss 1.0530 Accuracy 0.4685\n",
            "Saving checkpoint for epoch 6 at ./drive/My Drive/nlp/ckpt/ckpt-7\n",
            "Time taken for 1 epoch: 1799.1977937221527 secs\n",
            "\n",
            "Start of epoch 7\n",
            "Epoch 7 Batch 0 Loss 1.3773 Accuracy 0.4079\n",
            "Epoch 7 Batch 50 Loss 1.2429 Accuracy 0.4418\n",
            "Epoch 7 Batch 100 Loss 1.2218 Accuracy 0.4445\n",
            "Epoch 7 Batch 150 Loss 1.2166 Accuracy 0.4449\n",
            "Epoch 7 Batch 200 Loss 1.2117 Accuracy 0.4463\n",
            "Epoch 7 Batch 250 Loss 1.2103 Accuracy 0.4466\n",
            "Epoch 7 Batch 300 Loss 1.2087 Accuracy 0.4467\n",
            "Epoch 7 Batch 350 Loss 1.2053 Accuracy 0.4476\n",
            "Epoch 7 Batch 400 Loss 1.2011 Accuracy 0.4472\n",
            "Epoch 7 Batch 450 Loss 1.1973 Accuracy 0.4477\n",
            "Epoch 7 Batch 500 Loss 1.1965 Accuracy 0.4481\n",
            "Epoch 7 Batch 550 Loss 1.1946 Accuracy 0.4479\n",
            "Epoch 7 Batch 600 Loss 1.1932 Accuracy 0.4477\n",
            "Epoch 7 Batch 650 Loss 1.1905 Accuracy 0.4477\n",
            "Epoch 7 Batch 700 Loss 1.1878 Accuracy 0.4485\n",
            "Epoch 7 Batch 750 Loss 1.1844 Accuracy 0.4490\n",
            "Epoch 7 Batch 800 Loss 1.1802 Accuracy 0.4496\n",
            "Epoch 7 Batch 850 Loss 1.1754 Accuracy 0.4500\n",
            "Epoch 7 Batch 900 Loss 1.1700 Accuracy 0.4503\n",
            "Epoch 7 Batch 950 Loss 1.1639 Accuracy 0.4511\n",
            "Epoch 7 Batch 1000 Loss 1.1577 Accuracy 0.4516\n",
            "Epoch 7 Batch 1050 Loss 1.1528 Accuracy 0.4520\n",
            "Epoch 7 Batch 1100 Loss 1.1486 Accuracy 0.4527\n",
            "Epoch 7 Batch 1150 Loss 1.1445 Accuracy 0.4531\n",
            "Epoch 7 Batch 1200 Loss 1.1394 Accuracy 0.4540\n",
            "Epoch 7 Batch 1250 Loss 1.1351 Accuracy 0.4549\n",
            "Epoch 7 Batch 1300 Loss 1.1312 Accuracy 0.4555\n",
            "Epoch 7 Batch 1350 Loss 1.1274 Accuracy 0.4563\n",
            "Epoch 7 Batch 1400 Loss 1.1230 Accuracy 0.4571\n",
            "Epoch 7 Batch 1450 Loss 1.1197 Accuracy 0.4577\n",
            "Epoch 7 Batch 1500 Loss 1.1160 Accuracy 0.4584\n",
            "Epoch 7 Batch 1550 Loss 1.1117 Accuracy 0.4590\n",
            "Epoch 7 Batch 1600 Loss 1.1085 Accuracy 0.4597\n",
            "Epoch 7 Batch 1650 Loss 1.1057 Accuracy 0.4603\n",
            "Epoch 7 Batch 1700 Loss 1.1023 Accuracy 0.4610\n",
            "Epoch 7 Batch 1750 Loss 1.0991 Accuracy 0.4618\n",
            "Epoch 7 Batch 1800 Loss 1.0968 Accuracy 0.4623\n",
            "Epoch 7 Batch 1850 Loss 1.0938 Accuracy 0.4630\n",
            "Epoch 7 Batch 1900 Loss 1.0911 Accuracy 0.4635\n",
            "Epoch 7 Batch 1950 Loss 1.0889 Accuracy 0.4641\n",
            "Epoch 7 Batch 2000 Loss 1.0861 Accuracy 0.4646\n",
            "Epoch 7 Batch 2050 Loss 1.0838 Accuracy 0.4652\n",
            "Epoch 7 Batch 2100 Loss 1.0809 Accuracy 0.4656\n",
            "Epoch 7 Batch 2150 Loss 1.0779 Accuracy 0.4659\n",
            "Epoch 7 Batch 2200 Loss 1.0746 Accuracy 0.4662\n",
            "Epoch 7 Batch 2250 Loss 1.0714 Accuracy 0.4664\n",
            "Epoch 7 Batch 2300 Loss 1.0687 Accuracy 0.4667\n",
            "Epoch 7 Batch 2350 Loss 1.0651 Accuracy 0.4669\n",
            "Epoch 7 Batch 2400 Loss 1.0614 Accuracy 0.4670\n",
            "Epoch 7 Batch 2450 Loss 1.0581 Accuracy 0.4673\n",
            "Epoch 7 Batch 2500 Loss 1.0549 Accuracy 0.4676\n",
            "Epoch 7 Batch 2550 Loss 1.0520 Accuracy 0.4679\n",
            "Epoch 7 Batch 2600 Loss 1.0492 Accuracy 0.4681\n",
            "Epoch 7 Batch 2650 Loss 1.0466 Accuracy 0.4684\n",
            "Epoch 7 Batch 2700 Loss 1.0444 Accuracy 0.4686\n",
            "Epoch 7 Batch 2750 Loss 1.0420 Accuracy 0.4690\n",
            "Epoch 7 Batch 2800 Loss 1.0395 Accuracy 0.4692\n",
            "Epoch 7 Batch 2850 Loss 1.0371 Accuracy 0.4694\n",
            "Epoch 7 Batch 2900 Loss 1.0348 Accuracy 0.4696\n",
            "Epoch 7 Batch 2950 Loss 1.0327 Accuracy 0.4700\n",
            "Epoch 7 Batch 3000 Loss 1.0302 Accuracy 0.4703\n",
            "Epoch 7 Batch 3050 Loss 1.0278 Accuracy 0.4706\n",
            "Epoch 7 Batch 3100 Loss 1.0259 Accuracy 0.4708\n",
            "Epoch 7 Batch 3150 Loss 1.0240 Accuracy 0.4712\n",
            "Epoch 7 Batch 3200 Loss 1.0220 Accuracy 0.4716\n",
            "Epoch 7 Batch 3250 Loss 1.0200 Accuracy 0.4719\n",
            "Epoch 7 Batch 3300 Loss 1.0178 Accuracy 0.4721\n",
            "Epoch 7 Batch 3350 Loss 1.0159 Accuracy 0.4724\n",
            "Epoch 7 Batch 3400 Loss 1.0141 Accuracy 0.4727\n",
            "Epoch 7 Batch 3450 Loss 1.0119 Accuracy 0.4730\n",
            "Epoch 7 Batch 3500 Loss 1.0098 Accuracy 0.4733\n",
            "Epoch 7 Batch 3550 Loss 1.0084 Accuracy 0.4737\n",
            "Epoch 7 Batch 3600 Loss 1.0068 Accuracy 0.4740\n",
            "Epoch 7 Batch 3650 Loss 1.0054 Accuracy 0.4743\n",
            "Epoch 7 Batch 3700 Loss 1.0039 Accuracy 0.4746\n",
            "Epoch 7 Batch 3750 Loss 1.0018 Accuracy 0.4749\n",
            "Epoch 7 Batch 3800 Loss 1.0003 Accuracy 0.4752\n",
            "Epoch 7 Batch 3850 Loss 0.9986 Accuracy 0.4756\n",
            "Epoch 7 Batch 3900 Loss 0.9971 Accuracy 0.4759\n",
            "Epoch 7 Batch 3950 Loss 0.9955 Accuracy 0.4762\n",
            "Epoch 7 Batch 4000 Loss 0.9936 Accuracy 0.4766\n",
            "Epoch 7 Batch 4050 Loss 0.9919 Accuracy 0.4769\n",
            "Epoch 7 Batch 4100 Loss 0.9901 Accuracy 0.4773\n",
            "Epoch 7 Batch 4150 Loss 0.9888 Accuracy 0.4776\n",
            "Epoch 7 Batch 4200 Loss 0.9873 Accuracy 0.4780\n",
            "Epoch 7 Batch 4250 Loss 0.9858 Accuracy 0.4783\n",
            "Epoch 7 Batch 4300 Loss 0.9845 Accuracy 0.4786\n",
            "Epoch 7 Batch 4350 Loss 0.9832 Accuracy 0.4789\n",
            "Epoch 7 Batch 4400 Loss 0.9818 Accuracy 0.4792\n",
            "Epoch 7 Batch 4450 Loss 0.9805 Accuracy 0.4796\n",
            "Epoch 7 Batch 4500 Loss 0.9795 Accuracy 0.4798\n",
            "Epoch 7 Batch 4550 Loss 0.9795 Accuracy 0.4799\n",
            "Epoch 7 Batch 4600 Loss 0.9798 Accuracy 0.4799\n",
            "Epoch 7 Batch 4650 Loss 0.9803 Accuracy 0.4799\n",
            "Epoch 7 Batch 4700 Loss 0.9814 Accuracy 0.4797\n",
            "Epoch 7 Batch 4750 Loss 0.9825 Accuracy 0.4796\n",
            "Epoch 7 Batch 4800 Loss 0.9838 Accuracy 0.4794\n",
            "Epoch 7 Batch 4850 Loss 0.9853 Accuracy 0.4792\n",
            "Epoch 7 Batch 4900 Loss 0.9872 Accuracy 0.4789\n",
            "Epoch 7 Batch 4950 Loss 0.9889 Accuracy 0.4786\n",
            "Epoch 7 Batch 5000 Loss 0.9909 Accuracy 0.4784\n",
            "Epoch 7 Batch 5050 Loss 0.9926 Accuracy 0.4781\n",
            "Epoch 7 Batch 5100 Loss 0.9943 Accuracy 0.4778\n",
            "Epoch 7 Batch 5150 Loss 0.9962 Accuracy 0.4776\n",
            "Epoch 7 Batch 5200 Loss 0.9980 Accuracy 0.4774\n",
            "Epoch 7 Batch 5250 Loss 0.9996 Accuracy 0.4771\n",
            "Epoch 7 Batch 5300 Loss 1.0013 Accuracy 0.4769\n",
            "Epoch 7 Batch 5350 Loss 1.0030 Accuracy 0.4766\n",
            "Epoch 7 Batch 5400 Loss 1.0045 Accuracy 0.4764\n",
            "Epoch 7 Batch 5450 Loss 1.0060 Accuracy 0.4761\n",
            "Epoch 7 Batch 5500 Loss 1.0078 Accuracy 0.4759\n",
            "Epoch 7 Batch 5550 Loss 1.0097 Accuracy 0.4756\n",
            "Epoch 7 Batch 5600 Loss 1.0113 Accuracy 0.4754\n",
            "Epoch 7 Batch 5650 Loss 1.0130 Accuracy 0.4751\n",
            "Epoch 7 Batch 5700 Loss 1.0150 Accuracy 0.4748\n",
            "Epoch 7 Batch 5750 Loss 1.0167 Accuracy 0.4745\n",
            "Epoch 7 Batch 5800 Loss 1.0182 Accuracy 0.4743\n",
            "Epoch 7 Batch 5850 Loss 1.0196 Accuracy 0.4740\n",
            "Epoch 7 Batch 5900 Loss 1.0210 Accuracy 0.4737\n",
            "Epoch 7 Batch 5950 Loss 1.0223 Accuracy 0.4735\n",
            "Epoch 7 Batch 6000 Loss 1.0238 Accuracy 0.4732\n",
            "Epoch 7 Batch 6050 Loss 1.0252 Accuracy 0.4730\n",
            "Epoch 7 Batch 6100 Loss 1.0264 Accuracy 0.4727\n",
            "Epoch 7 Batch 6150 Loss 1.0277 Accuracy 0.4725\n",
            "Saving checkpoint for epoch 7 at ./drive/My Drive/nlp/ckpt/ckpt-8\n",
            "Time taken for 1 epoch: 1807.138727426529 secs\n",
            "\n",
            "Start of epoch 8\n",
            "Epoch 8 Batch 0 Loss 1.0889 Accuracy 0.4852\n",
            "Epoch 8 Batch 50 Loss 1.2151 Accuracy 0.4524\n",
            "Epoch 8 Batch 100 Loss 1.2061 Accuracy 0.4513\n",
            "Epoch 8 Batch 150 Loss 1.1967 Accuracy 0.4513\n",
            "Epoch 8 Batch 200 Loss 1.1945 Accuracy 0.4516\n",
            "Epoch 8 Batch 250 Loss 1.1912 Accuracy 0.4512\n",
            "Epoch 8 Batch 300 Loss 1.1920 Accuracy 0.4509\n",
            "Epoch 8 Batch 350 Loss 1.1849 Accuracy 0.4518\n",
            "Epoch 8 Batch 400 Loss 1.1811 Accuracy 0.4516\n",
            "Epoch 8 Batch 450 Loss 1.1755 Accuracy 0.4517\n",
            "Epoch 8 Batch 500 Loss 1.1736 Accuracy 0.4515\n",
            "Epoch 8 Batch 550 Loss 1.1706 Accuracy 0.4517\n",
            "Epoch 8 Batch 600 Loss 1.1677 Accuracy 0.4514\n",
            "Epoch 8 Batch 650 Loss 1.1670 Accuracy 0.4515\n",
            "Epoch 8 Batch 700 Loss 1.1644 Accuracy 0.4523\n",
            "Epoch 8 Batch 750 Loss 1.1602 Accuracy 0.4526\n",
            "Epoch 8 Batch 800 Loss 1.1560 Accuracy 0.4533\n",
            "Epoch 8 Batch 850 Loss 1.1531 Accuracy 0.4538\n",
            "Epoch 8 Batch 900 Loss 1.1477 Accuracy 0.4540\n",
            "Epoch 8 Batch 950 Loss 1.1415 Accuracy 0.4545\n",
            "Epoch 8 Batch 1000 Loss 1.1380 Accuracy 0.4551\n",
            "Epoch 8 Batch 1050 Loss 1.1324 Accuracy 0.4558\n",
            "Epoch 8 Batch 1100 Loss 1.1274 Accuracy 0.4565\n",
            "Epoch 8 Batch 1150 Loss 1.1228 Accuracy 0.4572\n",
            "Epoch 8 Batch 1200 Loss 1.1182 Accuracy 0.4579\n",
            "Epoch 8 Batch 1250 Loss 1.1132 Accuracy 0.4585\n",
            "Epoch 8 Batch 1300 Loss 1.1084 Accuracy 0.4592\n",
            "Epoch 8 Batch 1350 Loss 1.1046 Accuracy 0.4599\n",
            "Epoch 8 Batch 1400 Loss 1.1010 Accuracy 0.4603\n",
            "Epoch 8 Batch 1450 Loss 1.0974 Accuracy 0.4609\n",
            "Epoch 8 Batch 1500 Loss 1.0942 Accuracy 0.4615\n",
            "Epoch 8 Batch 1550 Loss 1.0903 Accuracy 0.4623\n",
            "Epoch 8 Batch 1600 Loss 1.0871 Accuracy 0.4630\n",
            "Epoch 8 Batch 1650 Loss 1.0840 Accuracy 0.4638\n",
            "Epoch 8 Batch 1700 Loss 1.0805 Accuracy 0.4645\n",
            "Epoch 8 Batch 1750 Loss 1.0772 Accuracy 0.4652\n",
            "Epoch 8 Batch 1800 Loss 1.0745 Accuracy 0.4657\n",
            "Epoch 8 Batch 1850 Loss 1.0723 Accuracy 0.4664\n",
            "Epoch 8 Batch 1900 Loss 1.0692 Accuracy 0.4669\n",
            "Epoch 8 Batch 1950 Loss 1.0666 Accuracy 0.4674\n",
            "Epoch 8 Batch 2000 Loss 1.0641 Accuracy 0.4678\n",
            "Epoch 8 Batch 2050 Loss 1.0611 Accuracy 0.4684\n",
            "Epoch 8 Batch 2100 Loss 1.0579 Accuracy 0.4688\n",
            "Epoch 8 Batch 2150 Loss 1.0555 Accuracy 0.4692\n",
            "Epoch 8 Batch 2200 Loss 1.0525 Accuracy 0.4694\n",
            "Epoch 8 Batch 2250 Loss 1.0498 Accuracy 0.4698\n",
            "Epoch 8 Batch 2300 Loss 1.0462 Accuracy 0.4700\n",
            "Epoch 8 Batch 2350 Loss 1.0428 Accuracy 0.4702\n",
            "Epoch 8 Batch 2400 Loss 1.0396 Accuracy 0.4704\n",
            "Epoch 8 Batch 2450 Loss 1.0364 Accuracy 0.4706\n",
            "Epoch 8 Batch 2500 Loss 1.0333 Accuracy 0.4707\n",
            "Epoch 8 Batch 2550 Loss 1.0304 Accuracy 0.4710\n",
            "Epoch 8 Batch 2600 Loss 1.0279 Accuracy 0.4712\n",
            "Epoch 8 Batch 2650 Loss 1.0252 Accuracy 0.4715\n",
            "Epoch 8 Batch 2700 Loss 1.0228 Accuracy 0.4718\n",
            "Epoch 8 Batch 2750 Loss 1.0203 Accuracy 0.4720\n",
            "Epoch 8 Batch 2800 Loss 1.0182 Accuracy 0.4722\n",
            "Epoch 8 Batch 2850 Loss 1.0156 Accuracy 0.4725\n",
            "Epoch 8 Batch 2900 Loss 1.0137 Accuracy 0.4728\n",
            "Epoch 8 Batch 2950 Loss 1.0111 Accuracy 0.4731\n",
            "Epoch 8 Batch 3000 Loss 1.0090 Accuracy 0.4735\n",
            "Epoch 8 Batch 3050 Loss 1.0066 Accuracy 0.4738\n",
            "Epoch 8 Batch 3100 Loss 1.0045 Accuracy 0.4741\n",
            "Epoch 8 Batch 3150 Loss 1.0024 Accuracy 0.4745\n",
            "Epoch 8 Batch 3200 Loss 1.0007 Accuracy 0.4747\n",
            "Epoch 8 Batch 3250 Loss 0.9985 Accuracy 0.4751\n",
            "Epoch 8 Batch 3300 Loss 0.9966 Accuracy 0.4754\n",
            "Epoch 8 Batch 3350 Loss 0.9951 Accuracy 0.4757\n",
            "Epoch 8 Batch 3400 Loss 0.9932 Accuracy 0.4760\n",
            "Epoch 8 Batch 3450 Loss 0.9912 Accuracy 0.4763\n",
            "Epoch 8 Batch 3500 Loss 0.9892 Accuracy 0.4766\n",
            "Epoch 8 Batch 3550 Loss 0.9873 Accuracy 0.4770\n",
            "Epoch 8 Batch 3600 Loss 0.9856 Accuracy 0.4772\n",
            "Epoch 8 Batch 3650 Loss 0.9838 Accuracy 0.4775\n",
            "Epoch 8 Batch 3700 Loss 0.9821 Accuracy 0.4779\n",
            "Epoch 8 Batch 3750 Loss 0.9803 Accuracy 0.4781\n",
            "Epoch 8 Batch 3800 Loss 0.9791 Accuracy 0.4783\n",
            "Epoch 8 Batch 3850 Loss 0.9778 Accuracy 0.4787\n",
            "Epoch 8 Batch 3900 Loss 0.9761 Accuracy 0.4790\n",
            "Epoch 8 Batch 3950 Loss 0.9743 Accuracy 0.4793\n",
            "Epoch 8 Batch 4000 Loss 0.9726 Accuracy 0.4797\n",
            "Epoch 8 Batch 4050 Loss 0.9712 Accuracy 0.4800\n",
            "Epoch 8 Batch 4100 Loss 0.9698 Accuracy 0.4803\n",
            "Epoch 8 Batch 4150 Loss 0.9684 Accuracy 0.4808\n",
            "Epoch 8 Batch 4200 Loss 0.9669 Accuracy 0.4811\n",
            "Epoch 8 Batch 4250 Loss 0.9655 Accuracy 0.4815\n",
            "Epoch 8 Batch 4300 Loss 0.9641 Accuracy 0.4818\n",
            "Epoch 8 Batch 4350 Loss 0.9628 Accuracy 0.4821\n",
            "Epoch 8 Batch 4400 Loss 0.9615 Accuracy 0.4823\n",
            "Epoch 8 Batch 4450 Loss 0.9604 Accuracy 0.4826\n",
            "Epoch 8 Batch 4500 Loss 0.9592 Accuracy 0.4829\n",
            "Epoch 8 Batch 4550 Loss 0.9591 Accuracy 0.4830\n",
            "Epoch 8 Batch 4600 Loss 0.9593 Accuracy 0.4830\n",
            "Epoch 8 Batch 4650 Loss 0.9597 Accuracy 0.4829\n",
            "Epoch 8 Batch 4700 Loss 0.9604 Accuracy 0.4829\n",
            "Epoch 8 Batch 4750 Loss 0.9615 Accuracy 0.4827\n",
            "Epoch 8 Batch 4800 Loss 0.9629 Accuracy 0.4825\n",
            "Epoch 8 Batch 4850 Loss 0.9645 Accuracy 0.4823\n",
            "Epoch 8 Batch 4900 Loss 0.9662 Accuracy 0.4820\n",
            "Epoch 8 Batch 4950 Loss 0.9681 Accuracy 0.4818\n",
            "Epoch 8 Batch 5000 Loss 0.9696 Accuracy 0.4815\n",
            "Epoch 8 Batch 5050 Loss 0.9713 Accuracy 0.4812\n",
            "Epoch 8 Batch 5100 Loss 0.9733 Accuracy 0.4809\n",
            "Epoch 8 Batch 5150 Loss 0.9752 Accuracy 0.4807\n",
            "Epoch 8 Batch 5200 Loss 0.9770 Accuracy 0.4804\n",
            "Epoch 8 Batch 5250 Loss 0.9787 Accuracy 0.4802\n",
            "Epoch 8 Batch 5300 Loss 0.9804 Accuracy 0.4800\n",
            "Epoch 8 Batch 5350 Loss 0.9823 Accuracy 0.4798\n",
            "Epoch 8 Batch 5400 Loss 0.9840 Accuracy 0.4795\n",
            "Epoch 8 Batch 5450 Loss 0.9858 Accuracy 0.4793\n",
            "Epoch 8 Batch 5500 Loss 0.9874 Accuracy 0.4791\n",
            "Epoch 8 Batch 5550 Loss 0.9891 Accuracy 0.4788\n",
            "Epoch 8 Batch 5600 Loss 0.9907 Accuracy 0.4785\n",
            "Epoch 8 Batch 5650 Loss 0.9923 Accuracy 0.4783\n",
            "Epoch 8 Batch 5700 Loss 0.9943 Accuracy 0.4780\n",
            "Epoch 8 Batch 5750 Loss 0.9958 Accuracy 0.4777\n",
            "Epoch 8 Batch 5800 Loss 0.9975 Accuracy 0.4773\n",
            "Epoch 8 Batch 5850 Loss 0.9990 Accuracy 0.4771\n",
            "Epoch 8 Batch 5900 Loss 1.0007 Accuracy 0.4768\n",
            "Epoch 8 Batch 5950 Loss 1.0021 Accuracy 0.4765\n",
            "Epoch 8 Batch 6000 Loss 1.0035 Accuracy 0.4763\n",
            "Epoch 8 Batch 6050 Loss 1.0048 Accuracy 0.4761\n",
            "Epoch 8 Batch 6100 Loss 1.0060 Accuracy 0.4758\n",
            "Epoch 8 Batch 6150 Loss 1.0074 Accuracy 0.4756\n",
            "Saving checkpoint for epoch 8 at ./drive/My Drive/nlp/ckpt/ckpt-9\n",
            "Time taken for 1 epoch: 1790.771065711975 secs\n",
            "\n",
            "Start of epoch 9\n",
            "Epoch 9 Batch 0 Loss 1.2803 Accuracy 0.4243\n",
            "Epoch 9 Batch 50 Loss 1.1819 Accuracy 0.4523\n",
            "Epoch 9 Batch 100 Loss 1.1762 Accuracy 0.4513\n",
            "Epoch 9 Batch 150 Loss 1.1673 Accuracy 0.4519\n",
            "Epoch 9 Batch 200 Loss 1.1615 Accuracy 0.4526\n",
            "Epoch 9 Batch 250 Loss 1.1605 Accuracy 0.4537\n",
            "Epoch 9 Batch 300 Loss 1.1565 Accuracy 0.4540\n",
            "Epoch 9 Batch 350 Loss 1.1580 Accuracy 0.4546\n",
            "Epoch 9 Batch 400 Loss 1.1573 Accuracy 0.4547\n",
            "Epoch 9 Batch 450 Loss 1.1538 Accuracy 0.4547\n",
            "Epoch 9 Batch 500 Loss 1.1498 Accuracy 0.4551\n",
            "Epoch 9 Batch 550 Loss 1.1490 Accuracy 0.4547\n",
            "Epoch 9 Batch 600 Loss 1.1476 Accuracy 0.4551\n",
            "Epoch 9 Batch 650 Loss 1.1463 Accuracy 0.4552\n",
            "Epoch 9 Batch 700 Loss 1.1445 Accuracy 0.4554\n",
            "Epoch 9 Batch 750 Loss 1.1419 Accuracy 0.4559\n",
            "Epoch 9 Batch 800 Loss 1.1386 Accuracy 0.4566\n",
            "Epoch 9 Batch 850 Loss 1.1351 Accuracy 0.4571\n",
            "Epoch 9 Batch 900 Loss 1.1305 Accuracy 0.4575\n",
            "Epoch 9 Batch 950 Loss 1.1251 Accuracy 0.4582\n",
            "Epoch 9 Batch 1000 Loss 1.1184 Accuracy 0.4588\n",
            "Epoch 9 Batch 1050 Loss 1.1131 Accuracy 0.4593\n",
            "Epoch 9 Batch 1100 Loss 1.1093 Accuracy 0.4599\n",
            "Epoch 9 Batch 1150 Loss 1.1041 Accuracy 0.4604\n",
            "Epoch 9 Batch 1200 Loss 1.0993 Accuracy 0.4609\n",
            "Epoch 9 Batch 1250 Loss 1.0943 Accuracy 0.4616\n",
            "Epoch 9 Batch 1300 Loss 1.0894 Accuracy 0.4623\n",
            "Epoch 9 Batch 1350 Loss 1.0857 Accuracy 0.4632\n",
            "Epoch 9 Batch 1400 Loss 1.0817 Accuracy 0.4639\n",
            "Epoch 9 Batch 1450 Loss 1.0777 Accuracy 0.4643\n",
            "Epoch 9 Batch 1500 Loss 1.0745 Accuracy 0.4650\n",
            "Epoch 9 Batch 1550 Loss 1.0712 Accuracy 0.4657\n",
            "Epoch 9 Batch 1600 Loss 1.0678 Accuracy 0.4665\n",
            "Epoch 9 Batch 1650 Loss 1.0643 Accuracy 0.4673\n",
            "Epoch 9 Batch 1700 Loss 1.0613 Accuracy 0.4677\n",
            "Epoch 9 Batch 1750 Loss 1.0588 Accuracy 0.4683\n",
            "Epoch 9 Batch 1800 Loss 1.0559 Accuracy 0.4690\n",
            "Epoch 9 Batch 1850 Loss 1.0535 Accuracy 0.4697\n",
            "Epoch 9 Batch 1900 Loss 1.0507 Accuracy 0.4701\n",
            "Epoch 9 Batch 1950 Loss 1.0477 Accuracy 0.4706\n",
            "Epoch 9 Batch 2000 Loss 1.0455 Accuracy 0.4710\n",
            "Epoch 9 Batch 2050 Loss 1.0428 Accuracy 0.4715\n",
            "Epoch 9 Batch 2100 Loss 1.0406 Accuracy 0.4720\n",
            "Epoch 9 Batch 2150 Loss 1.0380 Accuracy 0.4723\n",
            "Epoch 9 Batch 2200 Loss 1.0352 Accuracy 0.4727\n",
            "Epoch 9 Batch 2250 Loss 1.0321 Accuracy 0.4729\n",
            "Epoch 9 Batch 2300 Loss 1.0286 Accuracy 0.4731\n",
            "Epoch 9 Batch 2350 Loss 1.0251 Accuracy 0.4732\n",
            "Epoch 9 Batch 2400 Loss 1.0219 Accuracy 0.4735\n",
            "Epoch 9 Batch 2450 Loss 1.0185 Accuracy 0.4736\n",
            "Epoch 9 Batch 2500 Loss 1.0152 Accuracy 0.4737\n",
            "Epoch 9 Batch 2550 Loss 1.0128 Accuracy 0.4739\n",
            "Epoch 9 Batch 2600 Loss 1.0097 Accuracy 0.4742\n",
            "Epoch 9 Batch 2650 Loss 1.0072 Accuracy 0.4744\n",
            "Epoch 9 Batch 2700 Loss 1.0043 Accuracy 0.4747\n",
            "Epoch 9 Batch 2750 Loss 1.0021 Accuracy 0.4751\n",
            "Epoch 9 Batch 2800 Loss 1.0000 Accuracy 0.4754\n",
            "Epoch 9 Batch 2850 Loss 0.9975 Accuracy 0.4757\n",
            "Epoch 9 Batch 2900 Loss 0.9950 Accuracy 0.4761\n",
            "Epoch 9 Batch 2950 Loss 0.9933 Accuracy 0.4763\n",
            "Epoch 9 Batch 3000 Loss 0.9907 Accuracy 0.4768\n",
            "Epoch 9 Batch 3050 Loss 0.9884 Accuracy 0.4771\n",
            "Epoch 9 Batch 3100 Loss 0.9866 Accuracy 0.4773\n",
            "Epoch 9 Batch 3150 Loss 0.9848 Accuracy 0.4777\n",
            "Epoch 9 Batch 3200 Loss 0.9827 Accuracy 0.4779\n",
            "Epoch 9 Batch 3250 Loss 0.9808 Accuracy 0.4782\n",
            "Epoch 9 Batch 3300 Loss 0.9789 Accuracy 0.4784\n",
            "Epoch 9 Batch 3350 Loss 0.9773 Accuracy 0.4787\n",
            "Epoch 9 Batch 3400 Loss 0.9755 Accuracy 0.4791\n",
            "Epoch 9 Batch 3450 Loss 0.9736 Accuracy 0.4794\n",
            "Epoch 9 Batch 3500 Loss 0.9715 Accuracy 0.4796\n",
            "Epoch 9 Batch 3550 Loss 0.9696 Accuracy 0.4798\n",
            "Epoch 9 Batch 3600 Loss 0.9680 Accuracy 0.4802\n",
            "Epoch 9 Batch 3650 Loss 0.9660 Accuracy 0.4804\n",
            "Epoch 9 Batch 3700 Loss 0.9642 Accuracy 0.4807\n",
            "Epoch 9 Batch 3750 Loss 0.9624 Accuracy 0.4810\n",
            "Epoch 9 Batch 3800 Loss 0.9609 Accuracy 0.4813\n",
            "Epoch 9 Batch 3850 Loss 0.9597 Accuracy 0.4816\n",
            "Epoch 9 Batch 3900 Loss 0.9581 Accuracy 0.4820\n",
            "Epoch 9 Batch 3950 Loss 0.9567 Accuracy 0.4822\n",
            "Epoch 9 Batch 4000 Loss 0.9550 Accuracy 0.4825\n",
            "Epoch 9 Batch 4050 Loss 0.9536 Accuracy 0.4830\n",
            "Epoch 9 Batch 4100 Loss 0.9520 Accuracy 0.4833\n",
            "Epoch 9 Batch 4150 Loss 0.9504 Accuracy 0.4836\n",
            "Epoch 9 Batch 4200 Loss 0.9490 Accuracy 0.4840\n",
            "Epoch 9 Batch 4250 Loss 0.9477 Accuracy 0.4844\n",
            "Epoch 9 Batch 4300 Loss 0.9462 Accuracy 0.4847\n",
            "Epoch 9 Batch 4350 Loss 0.9451 Accuracy 0.4851\n",
            "Epoch 9 Batch 4400 Loss 0.9439 Accuracy 0.4854\n",
            "Epoch 9 Batch 4450 Loss 0.9426 Accuracy 0.4857\n",
            "Epoch 9 Batch 4500 Loss 0.9417 Accuracy 0.4859\n",
            "Epoch 9 Batch 4550 Loss 0.9416 Accuracy 0.4860\n",
            "Epoch 9 Batch 4600 Loss 0.9419 Accuracy 0.4861\n",
            "Epoch 9 Batch 4650 Loss 0.9423 Accuracy 0.4861\n",
            "Epoch 9 Batch 4700 Loss 0.9432 Accuracy 0.4859\n",
            "Epoch 9 Batch 4750 Loss 0.9442 Accuracy 0.4858\n",
            "Epoch 9 Batch 4800 Loss 0.9454 Accuracy 0.4855\n",
            "Epoch 9 Batch 4850 Loss 0.9470 Accuracy 0.4853\n",
            "Epoch 9 Batch 4900 Loss 0.9486 Accuracy 0.4851\n",
            "Epoch 9 Batch 4950 Loss 0.9503 Accuracy 0.4848\n",
            "Epoch 9 Batch 5000 Loss 0.9520 Accuracy 0.4845\n",
            "Epoch 9 Batch 5050 Loss 0.9539 Accuracy 0.4842\n",
            "Epoch 9 Batch 5100 Loss 0.9556 Accuracy 0.4839\n",
            "Epoch 9 Batch 5150 Loss 0.9573 Accuracy 0.4837\n",
            "Epoch 9 Batch 5200 Loss 0.9593 Accuracy 0.4834\n",
            "Epoch 9 Batch 5250 Loss 0.9613 Accuracy 0.4831\n",
            "Epoch 9 Batch 5300 Loss 0.9630 Accuracy 0.4829\n",
            "Epoch 9 Batch 5350 Loss 0.9649 Accuracy 0.4826\n",
            "Epoch 9 Batch 5400 Loss 0.9665 Accuracy 0.4824\n",
            "Epoch 9 Batch 5450 Loss 0.9682 Accuracy 0.4822\n",
            "Epoch 9 Batch 5500 Loss 0.9700 Accuracy 0.4819\n",
            "Epoch 9 Batch 5550 Loss 0.9719 Accuracy 0.4817\n",
            "Epoch 9 Batch 5600 Loss 0.9735 Accuracy 0.4814\n",
            "Epoch 9 Batch 5650 Loss 0.9754 Accuracy 0.4811\n",
            "Epoch 9 Batch 5700 Loss 0.9773 Accuracy 0.4809\n",
            "Epoch 9 Batch 5750 Loss 0.9789 Accuracy 0.4806\n",
            "Epoch 9 Batch 5800 Loss 0.9805 Accuracy 0.4803\n",
            "Epoch 9 Batch 5850 Loss 0.9821 Accuracy 0.4800\n",
            "Epoch 9 Batch 5900 Loss 0.9835 Accuracy 0.4797\n",
            "Epoch 9 Batch 5950 Loss 0.9852 Accuracy 0.4795\n",
            "Epoch 9 Batch 6000 Loss 0.9865 Accuracy 0.4792\n",
            "Epoch 9 Batch 6050 Loss 0.9879 Accuracy 0.4790\n",
            "Epoch 9 Batch 6100 Loss 0.9891 Accuracy 0.4787\n",
            "Epoch 9 Batch 6150 Loss 0.9905 Accuracy 0.4784\n",
            "Saving checkpoint for epoch 9 at ./drive/My Drive/nlp/ckpt/ckpt-10\n",
            "Time taken for 1 epoch: 1796.2202608585358 secs\n",
            "\n",
            "Start of epoch 10\n",
            "Epoch 10 Batch 0 Loss 1.1048 Accuracy 0.4548\n",
            "Epoch 10 Batch 50 Loss 1.1709 Accuracy 0.4537\n",
            "Epoch 10 Batch 100 Loss 1.1633 Accuracy 0.4552\n",
            "Epoch 10 Batch 150 Loss 1.1679 Accuracy 0.4556\n",
            "Epoch 10 Batch 200 Loss 1.1574 Accuracy 0.4558\n",
            "Epoch 10 Batch 250 Loss 1.1533 Accuracy 0.4566\n",
            "Epoch 10 Batch 300 Loss 1.1503 Accuracy 0.4567\n",
            "Epoch 10 Batch 350 Loss 1.1476 Accuracy 0.4564\n",
            "Epoch 10 Batch 400 Loss 1.1407 Accuracy 0.4566\n",
            "Epoch 10 Batch 450 Loss 1.1387 Accuracy 0.4570\n",
            "Epoch 10 Batch 500 Loss 1.1360 Accuracy 0.4571\n",
            "Epoch 10 Batch 550 Loss 1.1343 Accuracy 0.4575\n",
            "Epoch 10 Batch 600 Loss 1.1336 Accuracy 0.4576\n",
            "Epoch 10 Batch 650 Loss 1.1330 Accuracy 0.4577\n",
            "Epoch 10 Batch 700 Loss 1.1292 Accuracy 0.4580\n",
            "Epoch 10 Batch 750 Loss 1.1266 Accuracy 0.4582\n",
            "Epoch 10 Batch 800 Loss 1.1229 Accuracy 0.4586\n",
            "Epoch 10 Batch 850 Loss 1.1166 Accuracy 0.4594\n",
            "Epoch 10 Batch 900 Loss 1.1132 Accuracy 0.4599\n",
            "Epoch 10 Batch 950 Loss 1.1074 Accuracy 0.4601\n",
            "Epoch 10 Batch 1000 Loss 1.1023 Accuracy 0.4608\n",
            "Epoch 10 Batch 1050 Loss 1.0966 Accuracy 0.4612\n",
            "Epoch 10 Batch 1100 Loss 1.0912 Accuracy 0.4619\n",
            "Epoch 10 Batch 1150 Loss 1.0864 Accuracy 0.4627\n",
            "Epoch 10 Batch 1200 Loss 1.0823 Accuracy 0.4635\n",
            "Epoch 10 Batch 1250 Loss 1.0779 Accuracy 0.4643\n",
            "Epoch 10 Batch 1300 Loss 1.0736 Accuracy 0.4648\n",
            "Epoch 10 Batch 1350 Loss 1.0702 Accuracy 0.4653\n",
            "Epoch 10 Batch 1400 Loss 1.0663 Accuracy 0.4657\n",
            "Epoch 10 Batch 1450 Loss 1.0628 Accuracy 0.4664\n",
            "Epoch 10 Batch 1500 Loss 1.0588 Accuracy 0.4671\n",
            "Epoch 10 Batch 1550 Loss 1.0555 Accuracy 0.4678\n",
            "Epoch 10 Batch 1600 Loss 1.0516 Accuracy 0.4685\n",
            "Epoch 10 Batch 1650 Loss 1.0489 Accuracy 0.4691\n",
            "Epoch 10 Batch 1700 Loss 1.0454 Accuracy 0.4698\n",
            "Epoch 10 Batch 1750 Loss 1.0432 Accuracy 0.4704\n",
            "Epoch 10 Batch 1800 Loss 1.0406 Accuracy 0.4711\n",
            "Epoch 10 Batch 1850 Loss 1.0376 Accuracy 0.4717\n",
            "Epoch 10 Batch 1900 Loss 1.0347 Accuracy 0.4723\n",
            "Epoch 10 Batch 1950 Loss 1.0325 Accuracy 0.4726\n",
            "Epoch 10 Batch 2000 Loss 1.0297 Accuracy 0.4731\n",
            "Epoch 10 Batch 2050 Loss 1.0280 Accuracy 0.4735\n",
            "Epoch 10 Batch 2100 Loss 1.0256 Accuracy 0.4740\n",
            "Epoch 10 Batch 2150 Loss 1.0232 Accuracy 0.4744\n",
            "Epoch 10 Batch 2200 Loss 1.0203 Accuracy 0.4748\n",
            "Epoch 10 Batch 2250 Loss 1.0173 Accuracy 0.4752\n",
            "Epoch 10 Batch 2300 Loss 1.0139 Accuracy 0.4755\n",
            "Epoch 10 Batch 2350 Loss 1.0101 Accuracy 0.4757\n",
            "Epoch 10 Batch 2400 Loss 1.0072 Accuracy 0.4759\n",
            "Epoch 10 Batch 2450 Loss 1.0037 Accuracy 0.4760\n",
            "Epoch 10 Batch 2500 Loss 1.0010 Accuracy 0.4761\n",
            "Epoch 10 Batch 2550 Loss 0.9984 Accuracy 0.4763\n",
            "Epoch 10 Batch 2600 Loss 0.9958 Accuracy 0.4766\n",
            "Epoch 10 Batch 2650 Loss 0.9928 Accuracy 0.4768\n",
            "Epoch 10 Batch 2700 Loss 0.9904 Accuracy 0.4771\n",
            "Epoch 10 Batch 2750 Loss 0.9881 Accuracy 0.4773\n",
            "Epoch 10 Batch 2800 Loss 0.9859 Accuracy 0.4776\n",
            "Epoch 10 Batch 2850 Loss 0.9836 Accuracy 0.4780\n",
            "Epoch 10 Batch 2900 Loss 0.9811 Accuracy 0.4782\n",
            "Epoch 10 Batch 2950 Loss 0.9793 Accuracy 0.4786\n",
            "Epoch 10 Batch 3000 Loss 0.9766 Accuracy 0.4788\n",
            "Epoch 10 Batch 3050 Loss 0.9745 Accuracy 0.4792\n",
            "Epoch 10 Batch 3100 Loss 0.9725 Accuracy 0.4793\n",
            "Epoch 10 Batch 3150 Loss 0.9708 Accuracy 0.4796\n",
            "Epoch 10 Batch 3200 Loss 0.9687 Accuracy 0.4800\n",
            "Epoch 10 Batch 3250 Loss 0.9668 Accuracy 0.4803\n",
            "Epoch 10 Batch 3300 Loss 0.9648 Accuracy 0.4805\n",
            "Epoch 10 Batch 3350 Loss 0.9632 Accuracy 0.4809\n",
            "Epoch 10 Batch 3400 Loss 0.9614 Accuracy 0.4812\n",
            "Epoch 10 Batch 3450 Loss 0.9597 Accuracy 0.4815\n",
            "Epoch 10 Batch 3500 Loss 0.9578 Accuracy 0.4818\n",
            "Epoch 10 Batch 3550 Loss 0.9557 Accuracy 0.4821\n",
            "Epoch 10 Batch 3600 Loss 0.9542 Accuracy 0.4823\n",
            "Epoch 10 Batch 3650 Loss 0.9524 Accuracy 0.4826\n",
            "Epoch 10 Batch 3700 Loss 0.9508 Accuracy 0.4830\n",
            "Epoch 10 Batch 3750 Loss 0.9492 Accuracy 0.4833\n",
            "Epoch 10 Batch 3800 Loss 0.9478 Accuracy 0.4836\n",
            "Epoch 10 Batch 3850 Loss 0.9460 Accuracy 0.4839\n",
            "Epoch 10 Batch 3900 Loss 0.9444 Accuracy 0.4842\n",
            "Epoch 10 Batch 3950 Loss 0.9426 Accuracy 0.4845\n",
            "Epoch 10 Batch 4000 Loss 0.9411 Accuracy 0.4848\n",
            "Epoch 10 Batch 4050 Loss 0.9397 Accuracy 0.4851\n",
            "Epoch 10 Batch 4100 Loss 0.9380 Accuracy 0.4855\n",
            "Epoch 10 Batch 4150 Loss 0.9365 Accuracy 0.4858\n",
            "Epoch 10 Batch 4200 Loss 0.9352 Accuracy 0.4862\n",
            "Epoch 10 Batch 4250 Loss 0.9339 Accuracy 0.4864\n",
            "Epoch 10 Batch 4300 Loss 0.9327 Accuracy 0.4868\n",
            "Epoch 10 Batch 4350 Loss 0.9314 Accuracy 0.4871\n",
            "Epoch 10 Batch 4400 Loss 0.9303 Accuracy 0.4875\n",
            "Epoch 10 Batch 4450 Loss 0.9291 Accuracy 0.4878\n",
            "Epoch 10 Batch 4500 Loss 0.9281 Accuracy 0.4880\n",
            "Epoch 10 Batch 4550 Loss 0.9276 Accuracy 0.4881\n",
            "Epoch 10 Batch 4600 Loss 0.9275 Accuracy 0.4881\n",
            "Epoch 10 Batch 4650 Loss 0.9283 Accuracy 0.4881\n",
            "Epoch 10 Batch 4700 Loss 0.9292 Accuracy 0.4880\n",
            "Epoch 10 Batch 4750 Loss 0.9303 Accuracy 0.4878\n",
            "Epoch 10 Batch 4800 Loss 0.9319 Accuracy 0.4876\n",
            "Epoch 10 Batch 4850 Loss 0.9334 Accuracy 0.4874\n",
            "Epoch 10 Batch 4900 Loss 0.9351 Accuracy 0.4871\n",
            "Epoch 10 Batch 4950 Loss 0.9367 Accuracy 0.4869\n",
            "Epoch 10 Batch 5000 Loss 0.9384 Accuracy 0.4866\n",
            "Epoch 10 Batch 5050 Loss 0.9399 Accuracy 0.4864\n",
            "Epoch 10 Batch 5100 Loss 0.9420 Accuracy 0.4861\n",
            "Epoch 10 Batch 5150 Loss 0.9440 Accuracy 0.4859\n",
            "Epoch 10 Batch 5200 Loss 0.9459 Accuracy 0.4856\n",
            "Epoch 10 Batch 5250 Loss 0.9476 Accuracy 0.4853\n",
            "Epoch 10 Batch 5300 Loss 0.9496 Accuracy 0.4851\n",
            "Epoch 10 Batch 5350 Loss 0.9513 Accuracy 0.4849\n",
            "Epoch 10 Batch 5400 Loss 0.9530 Accuracy 0.4847\n",
            "Epoch 10 Batch 5450 Loss 0.9546 Accuracy 0.4844\n",
            "Epoch 10 Batch 5500 Loss 0.9563 Accuracy 0.4842\n",
            "Epoch 10 Batch 5550 Loss 0.9582 Accuracy 0.4839\n",
            "Epoch 10 Batch 5600 Loss 0.9599 Accuracy 0.4836\n",
            "Epoch 10 Batch 5650 Loss 0.9615 Accuracy 0.4833\n",
            "Epoch 10 Batch 5700 Loss 0.9630 Accuracy 0.4830\n",
            "Epoch 10 Batch 5750 Loss 0.9648 Accuracy 0.4827\n",
            "Epoch 10 Batch 5800 Loss 0.9663 Accuracy 0.4824\n",
            "Epoch 10 Batch 5850 Loss 0.9678 Accuracy 0.4821\n",
            "Epoch 10 Batch 5900 Loss 0.9695 Accuracy 0.4818\n",
            "Epoch 10 Batch 5950 Loss 0.9709 Accuracy 0.4815\n",
            "Epoch 10 Batch 6000 Loss 0.9723 Accuracy 0.4813\n",
            "Epoch 10 Batch 6050 Loss 0.9735 Accuracy 0.4811\n",
            "Epoch 10 Batch 6100 Loss 0.9750 Accuracy 0.4809\n",
            "Epoch 10 Batch 6150 Loss 0.9764 Accuracy 0.4806\n",
            "Saving checkpoint for epoch 10 at ./drive/My Drive/nlp/ckpt/ckpt-11\n",
            "Time taken for 1 epoch: 1847.1568927764893 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYF5PAlYhpM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluation\n",
        "def evaluate(inp_sentence):\n",
        "    inp_sentence = \\\n",
        "        [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
        "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
        "    \n",
        "    output = tf.expand_dims([VOCAB_SIZE_PT-2], axis=0)\n",
        "    \n",
        "    for _ in range(MAX_LENGTH):\n",
        "        predictions = transformer(enc_input, output, False)\n",
        "        \n",
        "        prediction = predictions[:, -1:, :]\n",
        "        \n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "        \n",
        "        if predicted_id == VOCAB_SIZE_PT-1:\n",
        "            return tf.squeeze(output, axis=0)\n",
        "        \n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "        \n",
        "    return tf.squeeze(output, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYghspXehw68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "    output = evaluate(sentence).numpy()\n",
        "    \n",
        "    predicted_sentence = tokenizer_pt.decode(\n",
        "        [i for i in output if i < VOCAB_SIZE_PT-2]\n",
        "    )\n",
        "    \n",
        "    print(\"Input: {}\".format(sentence))\n",
        "    print(\"Predicted translation: {}\".format(predicted_sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxLZ6wVZpsUf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3e438be4-0a12-42c5-d6db-ea188322cd43"
      },
      "source": [
        "translate(\"What a powerful tool!\")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: What a powerful tool!\n",
            "Predicted translation: Que instrumento poderoso!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZHcoewSp4fA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "05fb6547-5d88-4ee9-ce1e-efff7f812875"
      },
      "source": [
        "translate(\"Do you understand the power of this tool?\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: Do you understand the power of this tool?\n",
            "Predicted translation: Compreendem o poder deste instrumento?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPZBIAfJq7bU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d3ef382d-2e21-4f51-b25d-bf3db427bd71"
      },
      "source": [
        "translate(\"Can a machine translate languages?\")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: Can a machine translate languages?\n",
            "Predicted translation: Será possível uma máquina traduzir línguas em tradução?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31uNmv34rLmT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e4c17a37-2426-41d7-9084-b98483d70f00"
      },
      "source": [
        "translate(\"Well, I can!\")"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: Well, I can!\n",
            "Predicted translation: Pois bem, posso fazer!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj9jWxVCh03G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3affa8b3-31dd-49ea-b9fa-c5a056f7aa10"
      },
      "source": [
        "translate(\"I am almost there!\")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: I am almost there!\n",
            "Predicted translation: Estou quase presente!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}